---
title: "Forecast Reconciliation for Land Transfer Duty"
author:
- name: Hoang Do
email: vdoo0002@student.monash.edu
organization: The Department of Treasury and Finance
bibliography: references.bib
format: report-pdf
output:
  monash::report:
    fig_caption: yes
    fig_height: 3
    fig_width: 4
    includes:
      in_header: preamble.tex
    keep_tex: yes
    number_sections: yes
    citation_package: biblatex
    toc: true
    extra_dependencies: ["float"]
---

```{r, warning=F, message= F, include=F}
# Load libraries
library(FoReco)
library(tidyverse)
library(readxl)
library(fpp3)
library(urca)
library(plotly)
library(knitr)
library(here)
library(tseries)
library(ggplot2)

# Load UDF
source(here::here("Function/var_function.R"))
source(here::here("Function/vecm_function.R"))
source(here::here("Function/adjust_series_fun.R"))
source(here::here("Function/test_extract_fun.R"))

# Load RDA file
load(here::here("data/dtf_mape.RData"))
load(here::here("data/dtf_rmse.RData"))

# Load rmse RDA file
load(here::here("data/rmse_base_arima.RData"))
load(here::here("data/rmse_base_vecm.RData"))
load(here::here("data/rmse_cross_temp_arima.RData"))
load(here::here("data/rmse_cross_temp_vecm.RData"))
load(here::here("data/rmse_temp_arima.RData"))
load(here::here("data/rmse_temp_vecm.RData"))

# Load mape RDA file
load(here::here("data/mape_base_arima.RData"))
load(here::here("data/mape_base_vecm.RData"))
load(here::here("data/mape_cross_temp_arima.RData"))
load(here::here("data/mape_cross_temp_vecm.RData"))
load(here::here("data/mape_temp_arima.RData"))
load(here::here("data/mape_temp_vecm.RData"))


```

```{r, warning=F, message= F}
# Load ltd aggregate date
ltd_agg <- read_excel(here("data/LTD_new.xlsx"), sheet = 1) |>
  rename(Date = ...1,
         ltd = LTD,
         sales = SALES,
         hvi = HVI) |>
  dplyr::select(c(Date, ltd, sales, hvi))

# Load ltd unit data and join with aggregate data
ltd_unit <- read_excel(here("data/LTD_new.xlsx"), sheet = 2) |>
  rename(Date = ...1) |>
  dplyr::select(Date, ltd_total, ltd_nonres, ltd_comm, ltd_ind, ltd_other, ltd_res) |>
  left_join(ltd_agg, by = c("Date")) |>
  dplyr::select(-ltd)
```

```{r, warning=F, message= F}
# Tax revenue data 
tax_rev <- read_excel(here::here("data/tax_rev.xlsx"), sheet = 2) |>
  rename(Date = ...1)|>
  mutate(reliance = `Reliance on stamp duty` *100 ) |>
  dplyr::select(Date, reliance) |>
  mutate(Quarter = yearquarter(Date)) |>
  dplyr::select(-Date)  |>
  as_tsibble(index = Quarter) |>
  relocate(Quarter)
```


# Abstract
- Overview: Briefly summarize the purpose, methodology, key findings, and implications of the forecast.
- Key Recommendations: High-level actionable recommendations based on the forecast results.

# Introduction and background

The property sector plays a pivotal role in Australia's economy, accounting for 1 in 4 jobs indirectly and contributing around 13% of Gross Domestic Product (GDP). In the 2021 financial year, property sales totaled approximately $350 billion (**Real Estate Institute of Australia, 2021**). Land transfer duty, previously known as stamp duty, significantly impacts property transactions and the sector as a whole. A literature published by the New South Wales Treasury found that a 100 basis point (1%) cut in land transfer duty could boost property transactions by 10% (**Malakellis & Warlters, 2021**).

Land transfer duty is a tax applied to the "dutiable value" of a property being purchased or acquired, whether it is a first home or an investment property. The dutiable value is determined as either the property's purchase price or its market value, whichever is greater. Several factors influence the amount of duty paid, including the buyer's intended use of the property, foreign purchaser status, and eligibility for exemptions.


```{r}
#| message: False
#| warning: False
#| echo: False
#| label: fig-taxrevenue
#| fig-cap: "Dependence of Victoria's Tax Revenue on Land Transfer Duty"
#| fig-height: 3
#| fig-width: 4
tax_rev |> 
autoplot(reliance, color = "blue") +
geom_hline(yintercept = mean(tax_rev$reliance), color = "red") +
labs(x = "Date", y = "Percentage", title = " ") +
theme_minimal()
```


Additionally, Victoria’s tax revenue heavily relies on land transfer duty. As can be seen from Figure @fig-taxrevenue, over the past 20 years, on average, land transfer duty accounts for **27%** of Victoria's tax revenue. Despite its perceived inequity and various exemptions designed to aid homebuyers, abolishing this duty remains challenging due to the need for equivalent revenue replacement. If this duty is removed, the government will need to introduce one or more new taxes to generate equivalent revenue.

To balance the need for sufficient tax income without discouraging property transactions, *the Department of Treasury and Finance* (**DTF**) has been closely working with the government on policy adjustments, subsidies, exemptions, and restrictions. Accurate forecasts of land transfer duty, both short-term (*1 to 3 months*) and long-term (*12 months*), are essential for these decisions. 

This report introduces a new forecasting methodology aimed at improving the accuracy of these predictions. By employing forecast reconciliation and combining cross-sectional and temporal hierarchies, we aim to provide the *Department of Treasury and Finance* with more reliable forecasts to inform policy-making.

# Exploratory Data Analysis (EDA)

The data provided by the *Department of Treasury and Finance* is already well-processed and cleaned, with no missing values. Therefore, no further data cleaning is required.

## Descriptive Statistics

Based on summary statistics results from @tbl-sumstat, 

1. **Land Transfer Duty Overview**:
    - The total land transfer duty (**LTD**) reported has varied considerably over the past decade, ranging from approximately 286.2 million to over 1.013 billion over past . This variability indicates significant fluctuations in the real estate market activity and property value changes over the years.
    - Residential transactions dominate the LTD collection, contributing between 238.1 million and 761.4 million, representing the largest portion of land transfer duty. This suggests that residential real estate remains a vital part of the market, possibly reflecting trends in housing demand and price movements.
    - Non-residential transactions, though smaller than residential, still make a substantial contribution, with LTD from this sector ranging from about 42.9 million to 357.2 million. Within this, the commercial sector is the most significant, followed by industrial and other sectors (such as agricultural).

2. **Detailed Sector Analysis**:
    - **Commercial Sector**: With LTD ranging from 22.1 million to 132.4 million, this sector shows substantial activity, possibly reflecting economic growth, investment trends, and business expansions.
    - **Industrial Sector**: This sector has collected LTD between 5.2 million and 214.4 million, highlighting some large-scale industrial transactions or developments during certain periods.
    - **Other Sectors**: Including smaller sectors like agricultural, contributions range from 3.3 million to 74.5 million, indicating sporadic activity that may correspond with specific market or economic conditions.

3. **Economic Indicators**:
    - **Sales**: The volume of sales has fluctuated from 4,629 to 15,177, indicating periods of both high and low market activity. 
    
    - **Home Value Index (HVI)**: The HVI has ranged from 106.2 to 192.4 point, which is indicative of significant changes in home values. A higher HVI suggests increasing property values, which could contribute to higher LTD collections.

## Time series analysis {#sec-tsanalysis}


```{r, message = FALSE, warning= FALSE}
ltd_agg_ts <- ltd_agg |>
  mutate(Month = yearmonth(Date)) |>
  select(-Date) |>
  as_tsibble(index = Month) |>
  relocate(Month)

ltd_unit_ts <- ltd_unit %>%
  mutate(Month = yearmonth(Date)) %>%
  select(-Date) %>%
  as_tsibble(index = Month) %>%
  relocate(Month)
```


### Total Land Transfer Duty


```{r}
#| message: False
#| warning: False
#| echo: False
#| label: fig-trend
#| fig-cap: "Time plot of Land Transfer Duty in Victoria over time"
#| fig-align: 'center'
#| fig-height: 3
#| fig-width: 4

ltd_unit_ts |>
  mutate(ltd_tot_inM = ltd_total/1000000) |>
autoplot(ltd_tot_inM, color = "red") +
  labs(title = " ", x = "Date", y = "Amount (in $m)") +
  theme_minimal()
```


@fig-trend demonstrates an upward, non-linear trend, showing that the value of land transfer duty has generally increased over the ten-year period. Additionally, there is a noticeable increase in variability, particularly in recent years, indicating more pronounced fluctuations in the values. This increasing volatility suggests that a transformation of the data may be necessary to better analyze and model these trends.


```{r}
#| message: False
#| warning: False
#| echo: False
#| label: fig-sspattern
#| fig-cap: "Seasonal trend of Land Transfer Duty in Victoria"
#| fig-align: 'center'

ltd_unit_ts |>
  mutate(ltd_tot_inM = ltd_total/1000000) |>
  gg_season(ltd_tot_inM, labels = "both") +
  theme(legend.position = "none") +
  labs(x = "Date", y = "Amount (in $m)", title= " ")+
  theme_minimal()
```


Considering the seasonal patterns of land transfer duty, @fig-sspattern reveals that, apart from slight increases in June and December, there is no clear indication of a prominent seasonal pattern.


```{r}
#| message: False
#| warning: False
#| echo: False
#| label: fig-dcmp
#| fig-cap: "Decomposition of log of Land Transfer Duty in Victoria"
#| fig-align: 'center'
#| fig-height: 3
#| fig-width: 4
#| 
log_ltd_ts <- ltd_unit_ts |>
  mutate(log_ltd = log(ltd_total))

dcmp <- log_ltd_ts |>
  model(stl = STL(log_ltd))

## Plot all components
components(dcmp) |>
  autoplot() +
  theme_minimal()
```

```{r}
#| message: False
#| warning: False
#| echo: False
#| label: fig-ssadj
#| fig-cap: "Seasonally adjusted of log Land Transfer Duty in Victoria"
#| fig-align: 'center'
#| fig-height: 3
#| fig-width: 4
components(dcmp) |>
  as_tsibble() |>
  autoplot(log_ltd, colour = "gray") +
  geom_line(aes(y=season_adjust), colour = "#0072B2") +
  labs(x = "Date", y = "Amount", title = " ") +
  theme_minimal()
```


@fig-dcmp shows that there is a seasonal pattern for log transformed land transfer duty but the large grey bar in the seasonal panel shows that the variation in the seasonal component is smallest compared to the variation in the data.

Moreover, @fig-dcmp demonstrates that after applying a log transformation, the variability becomes more consistent, although some heterogeneity still remains.

The grey line in @fig-ssadj represents the original log-transformed total land transfer duty, while the blue line represents the seasonally adjusted log-transformed total land transfer duty. @fig-ssadj suggests that by adjusting for the seasonal pattern, some local peaks and troughs have been smoothed out, resulting in a more consistent series.

### Sales


```{r}
#| message: False
#| warning: False
#| echo: False
#| label: fig-salestrend
#| fig-cap: "Time plot of Sales in Victoria over time"
#| fig-align: 'center'
#| fig-height: 3
#| fig-width: 4
ltd_unit_ts |>
  autoplot(sales, color = "red") +
  labs(title = " ", x = "Date", y = "Unit") +
  theme_minimal()
```


@fig-salestrend illustrates a non-linear trend, showing fluctuations in sales units over the ten-year period from January 2014 to January 2024. The plot exhibits considerable variability with both high and low peaks scattered throughout the timeline. The values range from approximately 5000 to over 15000, indicating significant changes in sales units.

In recent years, there is a noticeable increase in variability, particularly around 2020 and onwards. This increasing volatility indicates that a transformation of the data may be necessary to better analyze and model these trends.


```{r}
#| message: False
#| warning: False
#| echo: False
#| label: fig-salessspattern
#| fig-cap: "Seasonal trend of Sales in Victoria"
#| fig-align: 'center'

ltd_unit_ts |>
  gg_season(sales, labels = "both") +
  theme(legend.position = "none") +
  labs(x = "Date", y = "Amount", title= " ") +
  theme(
    axis.text.y = element_text(size = 10, hjust = 1),
    axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
    plot.title = element_text(hjust = 0.5),
    legend.position = "right"
  ) +
  theme_minimal()
```


@fig-salessspattern depicts a noticeable peaks in the month of March, indicating higher sales during this month @fig-salessspattern also shows variability in the amount collected each year, with some years exhibiting more pronounced peaks and troughs. 


```{r}
#| message: False
#| warning: False
#| echo: False
#| label: fig-salesdcmp
#| fig-cap: "Decomposition of log of sales in Victoria"
#| fig-align: 'center'
#| fig-height: 3
#| fig-width: 4
log_ltd_ts <- ltd_unit_ts |>
  mutate(log_sales = log(sales))

dcmp <- log_ltd_ts |>
  model(stl = STL(log_sales))

## Plot all components
components(dcmp) |>
  autoplot() +
  theme_minimal()
```

```{r}
#| message: False
#| warning: False
#| echo: False
#| label: fig-salesssadj
#| fig-cap: "Seasonally adjusted of log of sales in Victoria"
#| fig-align: 'center'
#| fig-height: 3
#| fig-width: 4
components(dcmp) |>
  as_tsibble() |>
  autoplot(log_sales, colour = "gray") +
  geom_line(aes(y=season_adjust), colour = "#0072B2") +
  labs(x = "Date", y = "Amount", title = " ") +
  theme_minimal()
```


@fig-salesdcmp indicates a seasonal pattern for log-transformed sales. However, the large grey bar in the seasonal panel demonstrates that the variation in the seasonal component is the smallest compared to the overall variation in the data.

Moreover, @fig-salesdcmp demonstrates that after applying a log transformation, the variability becomes more consistent, although some heterogeneity still remains.

The grey line in @fig-salesssadj represents the original log-transformed sales, while the blue line represents the seasonally adjusted log-transformed sales. @fig-salesssadj suggests that there is a huge difference between seasonally adjusted sales and original sales, especially in periodic drops.


### Home Value Index


```{r}
#| message: False
#| warning: False
#| echo: False
#| label: fig-hvitrend
#| fig-cap: "Time plot of Home Value Index in Victoria over time"
#| fig-align: 'center'
#| fig-height: 3
#| fig-width: 4
ltd_unit_ts |>
  autoplot(hvi, color = "red") +
  labs(title = " ", x = "Date", y = "Index Point") +
  theme_minimal()
```


@fig-hvitrend illustrates an increasing trend, with some considerable variability throughout the timeline. 


```{r}
#| message: False
#| warning: False
#| echo: False
#| label: fig-hvisspattern
#| fig-cap: "Seasonal trend of Home Value Index in Victoria"
#| fig-align: 'center'

ltd_unit_ts |>
  gg_season(hvi, labels = "both") +
  theme(legend.position = "none") +
  labs(x = "Date", y = "Index Point", title= " ") +
  theme(
    axis.text.y = element_text(size = 10, hjust = 1),
    axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
    plot.title = element_text(hjust = 0.5),
    legend.position = "right"
  ) +
  theme_minimal()
```


@fig-hvisspattern depicts no clear sign of seasonal pattern in home value index, suggesting a stable index point maintained throughout the period.


```{r}
#| message: False
#| warning: False
#| echo: False
#| label: fig-hvidcmp
#| fig-cap: "Decomposition of log of home value index in Victoria"
#| fig-align: 'center'
#| fig-height: 3
#| fig-width: 4
log_ltd_ts <- ltd_unit_ts |>
  mutate(log_hvi = log(hvi))

dcmp <- log_ltd_ts |>
  model(stl = STL(log_hvi))

## Plot all components
components(dcmp) |>
  autoplot() +
  theme_minimal()
```

```{r}
#| message: False
#| warning: False
#| echo: False
#| label: fig-hvissadj
#| fig-cap: "Seasonally adjusted of log of home value index in Victoria"
#| fig-align: 'center'
#| fig-height: 3
#| fig-width: 4
components(dcmp) |>
  as_tsibble() |>
  autoplot(log_hvi, colour = "gray") +
  geom_line(aes(y=season_adjust), colour = "#0072B2") +
  labs(x = "Date", y = "Amount", title = " ") +
  theme_minimal()
```


@fig-hvidcmp indicates a seasonal pattern, but the large grey bar in the seasonal panel shows that the variation in the seasonal component is minimal, similar to the pattern observed in sales and land transfer duty.

In @fig-hvissadj, the blue and grey lines completely overlap, indicating that there is no seasonal pattern in the home value index.


## Cointegration Analysis {#sec-coint}

Based on the observations from @fig-trend, which shows the trend and increasing variance pattern in land transfer duty, @fig-salestrend, which exhibits an increasing pattern and a somewhat seasonal pattern in sales, and @fig-hvitrend, which illustrates the trend in the home value index, we can conclude that these three series are not stationary.

Furthermore, to justify the use of the Vector Error Correction Model (VECM) by the *Department of Treasury and Finance*, it is essential to test for the presence of cointegration patterns (or long-term equilibrium relationships) among these three series.

The Johansen procedure will be employed to test for cointegration between log transformed of  land transfer duty, sales, and the home value index series. This procedure tests the null hypothesis that no cointegration relationship exists among the series.


```{r, echo = F, warning = F, message = F}
jtest <- ca.jo(log(ltd_unit_ts[,c(2,8,9)]), type="trace", K=3, ecdet="none", spec="longrun")

jtest_sum <- summary(jtest)

jtest_sum
```


The test results, using 3 lags as chosen by the DTF, allow us to reject the null hypothesis that $r \leq 1$, yet we fail to reject the null hypothesis that $r \leq 2$ at 5% level of significance. This suggests that among the three variables, the rank of the matrix exceeds 2, indicating the presence of at least two cointegration relationships.

Furthermore, a rank of 2 implies that we need a combination of at least two time series to form a stationary series.

To create such a linear combination, we can utilize the components of the eigenvector associated with the largest eigenvalue. According to the Johansen test summary, the largest eigenvalue is approximately `r jtest_sum@lambda[1]`. This corresponds to the eigenvectors under the column `ltd_total.l3`, which is approximately equal to (`r jtest_sum@V[,1]`). By forming a linear combination of the series using these eigenvector components, we can achieve a stationary series.


```{r}
#| message: False
#| warning: False
#| echo: False
#| label: fig-splot
#| fig-cap: "Stationary series formed via a linear combination of 3 time series"
#| fig-align: 'center'
#| fig-height: 3
#| fig-width: 4

eigenvectors <- as.numeric(jtest_sum@V[,1])

s <- ltd_unit_ts |>
mutate(log_ltd = log(ltd_total), 
       log_sales = log(sales),
       log_hvi = log(hvi)) |> 
dplyr::select(c(log_ltd, log_sales, log_hvi)) |> 
summarise(s1 = log_ltd*eigenvectors[1] + eigenvectors[2]*log_sales + eigenvectors[3]*log_hvi) 

df <- data.frame(Date = s$Month, Value = as.numeric(unlist(s[,2])))

ggplot(df, aes(x = Date, y = Value)) + 
  geom_line(color = "red") + 
  labs(y = "Value", x = "Date", title = " ") +
theme_minimal()

```


As shown in @fig-splot, the linear combination appears to be more stationary, although there remains a slight indication of varying variances over time.

We can further evaluate this by applying the Augmented Dickey-Fuller (ADF) test:


```{r}
linear_comb <- as.numeric(unlist(s[,2]))
adf.test(linear_comb)
```


The ADF test statistic indicates that we can reject the null hypothesis of a unit root, providing evidence of a stationary series formed from the linear combination at the 10% level of significance. Therefore, we can conclude that it is appropriate to fit a Vector Error Correction Model (VECM) to these three time series.


# Methodology
## Cross-sectional, temporal and cross-temporal hierarchies

Upon analyzing the data characteristics and the disaggregation of LTD, it becomes evident that a three-level hierarchical structure can be established. There is utility in generating forecasts at various levels of aggregation, driven by diverse reasons and objectives. For example, each level of aggregation may exhibit distinct characteristics; for instance, transactions involving residential properties might vary from those involving non-residential properties due to differences in market dynamics or market size for each property type.

In this context, total land transfer duty is divided into two categories: residential and non-residential properties. Non-residential properties are further disaggregated into three sub-categories: commercial, industrial, and other, which predominantly includes agricultural properties. In addition to the total land transfer duty for all property types, the government and the *Department of Treasury and Finance* may also be interested in forecasts for each category and sub-category. @fig-crosssec illustrates the cross-sectional hierarchical structure of land transfer duty as discussed.

![Cross-sectional hierarchy](image/cross_sec.png){#fig-crosssec fig-alt="Cross-sectional hierarchy" fig-align="center" width=65%} 

Given that this is a straightforward cross-sectional hierarchical structure, we will also consider temporal hierarchies to further enhance forecast accuracy. Land transfer duty is collected monthly, and forecasts can be generated at bi-monthly, quarterly, four-monthly, semi-annual, and annual frequencies. Various temporal hierarchies can be constructed with monthly land transfer duty treated as the bottom level. @fig-temp presents an example of temporal hierarchies with monthly land transfer duty as the bottom level.

![Temporal hierarchy](image/temp.png){#fig-temp fig-align="center"}

Although forecasts using cross-sectional and temporal hierarchies have demonstrated substantial improvements (**Kourentzes & Athanasopoulos, 2019**), these approaches have typically been used separately. By combining cross-sectional hierarchies and temporal hierarchies, referred to as cross-temporal hierarchies, forecast accuracy can be further improved. @fig-crosstemp from **Kourentzes and Athanasopoulos, 2019** shows an example of a cross-temporal hierarchical structure.

![Cross-temporal hierarchy](image/cross-temporal.png){#fig-crosstemp fig-align="center"}

## Forecast reconciliation

From the cross-temporal hierarchies, forecasts can be produced at all levels for every nodes. And in an ideal scenario, forecasts from different levels of aggregation could seamlessly sum up to the top level. However, practical implementation often reveals incoherent among independently produced forecasts. Consequently, it becomes vital for forecasts to align and aggregate according to the hierarchical structure organizing the array of time series. As a solution to this challenge, forecast reconciliation emerges as one of the most effective methodologies employed today.

We first consider cross-sectional forecast reconciliation. Recall from @fig-crosssec, we can construct this hierarchical structure for LTD in a matrix form like this:

$$
\begin{bmatrix}
  \text{Total}_{t} \\
  \text{Non-residential}_{t} \\
  \text{Residential}_{t} \\
  \text{Commercial}_{t} \\
  \text{Industrial}_{t} \\
  \text{Other}_{t} \\
  \end{bmatrix}
=
\begin{bmatrix}
  1 & 1 & 1 & 1 \\
  0 & 1 & 1 & 1 \\
  1 & 0 & 0 & 0 \\
  0 & 1 & 0 & 0 \\
  0 & 0 & 1 & 0 \\
  0 & 0 & 0 & 1 \\
\end{bmatrix}
\begin{bmatrix}
  \text{Residential}_{t} \\
  \text{Commercial}_{t} \\
  \text{Industrial}_{t} \\
  \text{Other}_{t} \\
\end{bmatrix}
$$

or in a more compact notation:

$$
\textbf{y}_{t} = \textbf{S}\textbf{b}_{t},
$$

where S represents the summing matrix defining how bottom-level series are aggregated.

Then for any set of base forecast, denoted as $\bf{\hat{y}}_{h}$, where h is the forecast horizon, all coherent forecasting approaches, denoted as $\bf{\tilde{y}}_{h}$, can be represented as:

$$
\bf{\tilde{y}}_{h} = \bf{SG}\hat{\bf{y}}_{h},
$$
where $\bf{G}$ is a matrix that maps the base forecasts into the bottom level (**Hyndman & Athanasopoulos, 2021**).

The equation shows that pre-multiplying any set of base forecasts with $\bf{SG}$ will return a set of coherent forecasts. So the idea is to find that matrix $\bf{G}$ in such a way that it minimises the variance between your reconciled forecast and your original forecast

Within the domain of hierarchical time series forecasting, there are three traditional single level approaches for generating forecasts for hierarchical time series. The first, known as the *bottom-up* approach, initiates by producing forecasts for each series at the lowest level and subsequently aggregates these to generate forecasts for the upper levels of the hierarchy. For this approach, $\bf{G}$ can be defined as:

$$
\textbf{G}
=
\begin{bmatrix}
  0 & 0 & 1 & 0 & 0 & 0 \\
  0 & 0 & 0 & 1 & 0 & 0 \\
  0 & 0 & 0 & 0 & 1 & 0 \\
  0 & 0 & 0 & 0 & 0 & 1 \\
\end{bmatrix},
$$
where the first 2 columns zero out the base forecast of the series above the bottom level. 

Conversely, the *top-down* approach starts with a forecast at the highest level, which is then disaggregated to lower levels using predetermined proportions—typically based on historical data distributions (**Gross & Sohl, 1990**). For this approach, $\bf{G}$ can be defined as:

$$
\textbf{G}
=
\begin{bmatrix}
  \text{p}_{1} & 0 & 0 & 0 & 0 & 0 \\
  \text{p}_{2} & 0 & 0 & 0 & 0 & 0 \\
  \text{p}_{3} & 0 & 0 & 0 & 0 & 0 \\
  \text{p}_{4} & 0 & 0 & 0 & 0 & 0 \\
\end{bmatrix},
$$
where the first column includes the set of proportions that distribute the base forecasts of the top level to the bottom level

Lastly, the *middle-out* approach is a combination of both the bottom-up and top-down methods. 

However, the traditional single level approaches may have their limitations since only base forecast at one level is used. In response to this, **Wickramasuriya et al., 2019** introduced the *MinT* (Minimum Trace) optimal reconciliation methodology, which devises a **G** matrix aimed at minimizing the total forecast variance within the coherent forecast set.

**Wickramasuriya et al., 2019** show that the variance-covariance of the h-step-ahead coherent forecast errors is given by:
$$
\textbf{V}_{h} = Var[\textbf{y}_{T+h} - \bf{\tilde{y}}_\text{h}] = \textbf{SG}\textbf{W}_{h}\textbf{G'S'} ,
$$
where $\textbf{W}_{h} = Var[\textbf{y}_{T+h} - \bf{\hat{y}}_{h}]$ is the variance-covariance matrix of the corresponding base forecast errors.

**Wickramasuriya et al., 2019** also show that: 
$$
\textbf{G} = (\textbf{S}'\textbf{W}_{h}^{-1}\textbf{S})^{-1}\textbf{S}'\textbf{W}_{h}^{-1},
$$
minimises the trace of $\textbf{V}_{h}$ subject to **S** **G** **S** = **S**

Therefore, the optimally reconciled forecasts are given by:
$$
\bf{\tilde{y}}_{h} = \textbf{S}(\textbf{S}'\textbf{W}_{h}^{-1}\textbf{S})^{-1}\textbf{S}'\textbf{W}_{h}^{-1}\bf{\hat{y}}_{h}
$$
which refers as the **MinT**

Now, a challenge with G is that it requires an estimation of $\textbf{W}_{h}$, the forecast error variance of h-step-ahead base forecasts. There are four simplifying approximations in place that have been shown to work well:

1. **OLS (Hyndman et al., 2011)**: 
    $\mathbf{W}_{h} = \text{k}_{h}\mathbf{I}$ for all h, where $\text{k}_{h} > 0$ 

2. $\bf{WLS}_{S}$  **(Athanasopoulos et al. 2017)**:
    $\mathbf{W}_{h} = \text{k}_{h}\mathbf{\Lambda}$ for all h, where $\text{k}_{h} > 0$, $\mathbf{\Lambda} = diag(\textbf{S1})$, and $\bf{1}$ is  unit vector of dimension $\it{m}$ (the number of bottom-level series)
    
3. $\bf{WLS}_{V}$ **(Hyndman et al. 2016)**: 
    $\mathbf{W}_{h} = \text{k}_{h}\text{diag(}\mathbf{\hat{W}}_{1}\text{)}$ for all h, where $\text{k}_{h} > 0$,

$$
\mathbf{\hat{W}}_{1} = \frac{1}{T}\sum_{t=1}^{T}\textbf{e}_{t}\textbf{e'}_{t},
$$
and $\textbf{e}_{t}$ is an $\it{n}$-dimensional vector of residuals of the models that generated the base forecasts stacked in the same order as the data.

4. $\bf{MinT}_{S}$ **(Wickramasuriya et al., 2019)**: 
  $\mathbf{W}_{h} = \text{k}_{h}\mathbf{\hat{W}^{*}}_{1, D}$ for all h, where $\text{k}_{h} > 0$, and $\mathbf{\hat{W}^{*}}_{1, D} = \mathit{\lambda}\mathbf{\hat{W}^{*}}_{1, D} + \text{(1-} \mathit{\lambda})\mathbf{\hat{W}}_{1}$ is a shrinkage estimator with diagonal target $\mathbf{\hat{W}^{*}}_{1, D}$, a diagonal matrix comprising the diagonal entries of $\mathbf{\hat{W}}_{1}$, and $\mathit{\lambda}$ the shrinkage intensity parameter. 
  
By following the above procedure, we will get cross-sectionally coherent forecasts. Moreover, this procedure extends to temporal reconciliation and cross-temporal reconciliation.
  
### Cross-sectional reconciliation
For cross-sectional reconciliation, there are two main challenges, as stated by **Kourentzes and Athanasopoulos, 2019**: 

1. the size of the cross-sectional dimension of the hierarchy; and 
2. the heterogeneity of the series across, but also within levels. 

The size relates directly with estimation of $\mathbf{W}_{h}$, and therefore very large hierarchies, estimation of $\mathbf{W}_{h}$ can be computationally expensive. However, this is not the case for this hierarchical structure in @fig-crosssec. On the other hand, it is expected that there will be heterogeneity between each cross-sectional levels. Therefore, in this case, we choose not to apply structural scaling since it assumes that each of the bottom-level base forecasts has errors with equal variance $\text{k}_{h}$. 

### Temporal reconciliation

On the other hand, for temporal hierarchies, forecasts across all levels are for the same series, therefore it is safe to assume homogeneity between each level. On the other hand, following the arguments by Athanasopoulos et al. (2017). Therefore, the approximations for $\mathbf{W}_{h}$ for temporal reconciliation chosen will be *structural scaling*.

### Cross-temporal reconciliation

The cross-temporal reconciliation methodology used in this context is heuristic first-temporal-then-cross-sectional reconciliation, proposed by **Kourentzes and Athanasopoulos, 2019**. It is a 2-step approach, where step 1 includes reconciliation through temporal hierarchies for each single variable to achieve temporally coherence forecast (**THieFs**). Then for step 2, from previously **THieFs**, we generate k cross-sectional reconciliations, setting $\textbf{W}_{h} = \bf{\hat{W}}_{h,\ell}$, where $\ell = 1,2,..,k$, and k denotes the number of temporal aggregation levels. This results in reconciliation matrix $\textbf{SG}_{\ell}$ for each temporal aggregation level. And by averaging across these, we compose a consesus reconciliation matrix **SG**, where $\textbf{G} = \frac{1}{k}\sum_{\ell=1}^{k}\textbf{G}_{\ell}$, capturing the reconciliation consesus across all *k* temporal aggregation levels. The outcome are cross-temporally reconciled forecasts, which are coherent across both dimensions, at all scales. 

## Model Selection

As discussed in @sec-coint, it highlights the pertinence of employing both VAR and VECM to adequately model these relationships.

One limitation of these models is their reliance on ample data to produce reliable parameters. Monthly data spanning over a decade for LTD has been found to be sufficient. Nevertheless, the subsequent section on cross-validation will elaborate on the precise sizing of the training dataset required to ensure compatibility with the VAR and VECM models.

In addition to VAR and VECM, the Autoregressive Integrated Moving Average (ARIMA) model has also been employed, primarily for purposes of comparison and validation of improvements. The efficacy of this comparison will be assessed through time series cross-validation, utilizing various accuracy metrics such as:

* *Root Mean Square Error (RMSE)*, 
where the RMSE is computed as:

$\text{RMSE} = \sqrt{\frac{1}{n} \sum_{t=1}^{n} (y_t - \hat{y}_t)^2}$

* *Mean Absolute Percentage Error (MAPE)*
We also compute MAPE by defining the percentage error, to overcome scale-dependency of RMSE

$\text{MAPE} = \frac{1}{n} \sum_{t=1}^{n} \left| 100 \times \frac{y_t - \hat{y}_t}{y_t} \right|$

# Time Series Cross-Validation

## Definition and rationale

Time series cross-validation represents a sophisticated adaptation of the conventional training/test set approach for model selection, as stated by **Hyndman and Athanasopoulos, 2021**. This methodology is particularly well-suited to time series data because it exclusively includes observations from periods prior to those being forecasted, distinguishing it from traditional cross-validation techniques. 

In the scope of this project, time series cross-validation is employed to rigorously evaluate whether the VAR/VECM or ARIMA models yield more accurate forecasts for this dataset across forecasting horizons ranging from 1 to 12 steps ahead. Additionally, this approach is used to gauge the extent of improvement introduced by the reconciliation process in comparison to the b base forecasts. Another critical application of time series cross-validation in this context is to generate rolling forecasts over a 12-month period for various time intervals. This is essential for the *Department of Treasury and Finance* to analyze the effects of market dynamics or policy changes on land transfer duty. 

The initial training set will start with 108 monthly observations, equating to 9 years of data, and will roll forward by 1 month for each subsequent step. This process will generate 10 folds of training sets. While the number of observations in the initial training set can be lower, 108 months were chosen for code efficiency.

## Generating forecast

To enhance forecast accuracy, this methodology extends beyond a cross-sectional hierarchical structure to incorporate a temporal dimension, culminating in cross-temporal reconciliation forecasting. This approach integrates forecasts across different time structures, and its efficacy is evaluated by comparing it with cross-sectional and temporal reconciliation forecasts individually to determine any improvements.

Given the monthly frequency of the data, additional aggregation levels such as bi-monthly, quarterly, four-monthly, semi-annually, and annually are established, with the annual aggregation representing the top level of the temporal hierarchy.

Models are fitted at each cross-sectional level across these varying temporal frequencies. A loop is utilized to generate forecasts and residuals, which are subsequently organized into a matrix for each temporal frequency: monthly (denoted as k1), bi-monthly (k2), quarterly (k3), four-monthly (k4), semi-annually (k6), and annually (k12). These matrices are then collectively stored within a list.

This structured approach facilitates the fitting of different models tailored to each temporal frequency, allowing for adjustments in argument values as necessary. Moreover, if a distinct model is required for various cross-sectional levels, an `if` condition is employed within each temporal frequency loop to ensure this customization. The forecasts and residuals are then allocated to `base` and `res` data structures, respectively.

### ARIMA

The ARIMA model will be implemented across all levels of the temporal hierarchical structure and at every cross-sectional level. Since as discussed in @sec-tsanalysis, there is none to very slight hint of seasonal pattern across all 3 time series, we will pay more attention on non-seasonal ARIMA model. It is a combination of differencing with autoregression and a moving average model. The full model can be written as:

$y'_t = c + \phi_1 y'_{t-1} + \cdots + \phi_p y'_{t-p} + \theta_1 \varepsilon_{t-1} + \cdots + \theta_q \varepsilon_{t-q} + \varepsilon_t$,
where $y'_t$ is the differenced series, and the predictors on the R.H.S include both lagged values of $y'_t$ and lagged errors. We can call this $\textbf{ARIMA}(p, d, q)$ model, where

|  |  |  
|------:|:-----|
|  p  | order of the autoregressive part;  |
|  d  | degree of first differencing involved; |
|  q  | order of the moving average part. |

On the other hand, we can also include the seasonal pattern by using seasonal ARIMA. In this case, the model will extend to:

$$
\text{ARIMA} \quad 
\begin{array}{c}
(p, d, q) \quad (P, D, Q)_m \\
\underbrace{\phantom{(p, d, q)}} \quad \underbrace{\phantom{(P, D, Q)_m}} \\
\text{Non-seasonal part} \quad \text{Seasonal part} \\
\text{of the model} \quad \text{of the model}
\end{array},
$$
where $m$ is the seasonal period (e.g., number of observations per year)

To implement this, we can use the `auto.arima()` function from the `forecast` package. This function efficiently determines the optimal parameters for the autoregressive lag, differencing, and moving average components of the model by automatically adjusting them for the error terms.

### VAR/VECM

A vector autoregression model of order 1, VAR(1) between 2 time series can be represented with:

$$
\begin{aligned}
y_t &= \beta_{10} + \beta_{11} y_{t-1} + \beta_{12} x_{t-1} + \nu_t^y \\
x_t &= \beta_{20} + \beta_{21} y_{t-1} + \beta_{22} x_{t-1} + \nu_t^x
\end{aligned}
$$
Then the one-step-ahead forecasts are generated by:

$$
\begin{aligned}
\hat{y}_{T+1|T} &= \hat{\beta}_{10} + \hat{\beta}_{11} y_{T} + \hat{\beta}_{12} x_{T} \\
\hat{x}_{T+1|T} &= \hat{\beta}_{20} + \hat{\beta}_{21} y_{T} + \hat{\beta}_{22} x_{T}
\end{aligned}
$$

The VAR(1) equation can be extended to lag of 3 between 3 time series, which is our case:

$$
\begin{aligned}
y_t &= \beta_{10} + \beta_{11} y_{t-1} + \beta_{12} x_{t-1} + \beta_{13} z_{t-1} + \beta_{14} y_{t-2} + \beta_{15} x_{t-2} + \beta_{16} z_{t-2} + \beta_{17} y_{t-3} + \beta_{18} x_{t-3} + \beta_{19} z_{t-3} + \nu_t^y \\
x_t &= \beta_{20} + \beta_{21} y_{t-1} + \beta_{22} x_{t-1} + \beta_{23} z_{t-1} + \beta_{24} y_{t-2} + \beta_{25} x_{t-2} + \beta_{26} z_{t-2} + \beta_{27} y_{t-3} + \beta_{28} x_{t-3} + \beta_{29} z_{t-3} + \nu_t^x \\
z_t &= \beta_{30} + \beta_{31} y_{t-1} + \beta_{32} x_{t-1} + \beta_{33} z_{t-1} + \beta_{34} y_{t-2} + \beta_{35} x_{t-2} + \beta_{36} z_{t-2} + \beta_{37} y_{t-3} + \beta_{38} x_{t-3} + \beta_{39} z_{t-3} + \nu_t^z
\end{aligned},
$$
where $y_t$, $x_t$ and $z_t$ represent land transfer duty, sales and home value index.

Or in a more compact notation:

$$
\begin{aligned}
y_t &= \beta_{10} + \sum_{j=1}^{3} \left( \beta_{1j1} y_{t-j} + \beta_{1j2} x_{t-j} + \beta_{1j3} z_{t-j} \right) + \nu_t^y \\
x_t &= \beta_{20} + \sum_{j=1}^{3} \left( \beta_{2j1} y_{t-j} + \beta_{2j2} x_{t-j} + \beta_{2j3} z_{t-j} \right) + \nu_t^x \\
z_t &= \beta_{30} + \sum_{j=1}^{3} \left( \beta_{3j1} y_{t-j} + \beta_{3j2} x_{t-j} + \beta_{3j3} z_{t-j} \right) + \nu_t^z
\end{aligned}
$$

However, as discussed in @sec-coint, there are at least 2 cointegration patterns existing between 3 time series, fitting VECM will be more appropriate, where VECM is extended by adding error correction term to VAR in first difference.

Fitting VAR model and VECM to complex hierarchical structured data necessitates a more intricate approach, as there is no standard method readily available for this specific application. Additionally, these models require the incorporation of external variables, such as sales and the home value index (HVI). To address these challenges, the chosen strategy involves the creation of two separate user-defined functions for VAR and VECM. These functions are designed to process the input data and output both the mean point forecasts and residuals, mirroring the functionality provided by the `auto.arima()` function. 

Finally, the reconciliation procedure can be implemented using the `FoReco` package *(Di Fonzo & Girolimetto, 2022)*.

# Results
## Model Performance

```{r}
i = 1
## Create a data frame for plotting
rmse_base_data <- data.frame(
  h_step = 1:12,
  RMSE_ARIMA_base = RMSE_arima_h_base[i,],
  RMSE_VECM_base = RMSE_vecm_h_base[i,],
  rmse_ARIMA_rec = RMSE_arima_h_rec[i,],
  rmse_VECM_rec = RMSE_vecm_h_tcs_rec[i,],
  rmse_ARIMA_temp = RMSE_arima_h_temp_rec[i,],
  rmse_VECM_temp = RMSE_vecm_h_thf_rec[i,],
  rmse_DTF = fold_rmse$average_rmse
)

## Convert to long format
rmse_data_long <- rmse_base_data %>% 
  pivot_longer(cols = -h_step, names_to = "Method", values_to = "RMSE")

## Add columns for line type and method
rmse_data_long <- rmse_data_long %>%
  mutate(
    LineType = case_when(
      grepl("rec", Method) ~ "Reconciled",
      grepl("temp", Method) ~ "Temporal Reconciled",
      TRUE ~ "Base"
    ),
    MethodGroup = case_when(
      grepl("ARIMA", Method) ~ "ARIMA",
      grepl("VECM", Method) ~ "VECM",
      Method == "rmse_DTF" ~ "DTF"
    )
  )

## Create the plot with specified colors and line types
ggplot_rmse <- ggplot(rmse_data_long, aes(x = h_step, y = RMSE, colour = MethodGroup, linetype = LineType)) +
  geom_line(linewidth = 1.1) +
  scale_color_manual(values = c(
    "ARIMA" = "blue",
    "VECM" = "red",
    "DTF" = "green"
  )) +
  scale_linetype_manual(values = c(
    "Base" = "solid",
    "Reconciled" = "dashed",
    "Temporal Reconciled" = "dotted"
  )) +
  labs(x = "h-step Forecast", y = "RMSE Value", title = "Monthly total LTD base and reconciled forecast RMSE by Method") +
  scale_x_continuous(breaks = 1:12) +
  theme_minimal() +
  guides(
    colour = guide_legend(title = "Colour: Model", override.aes = list(linetype = "solid")),
    linetype = guide_legend(title = "Line type")
  )

ggplot_rmse

```

```{r}
i = 1
## Create a data frame for plotting
mape_base_data <- data.frame(
  h_step = 1:12,
  mape_arima_base = mape_arima_base[i,],
  mape_vecm_base = mape_vecm_base[i,],
  mape_arima_rec = mape_arima_cross_temp[i,],
  mape_vecm_rec = mape_vecm_cross_temp[i,],
  mape_arima_temp = mape_arima_temp[i,],
  mape_vecm_temp = mape_vecm_temp[i,],
  mape_DTF = fold_mape$average_mape
)

## Convert to long format
mape_data_long <- mape_base_data %>% 
  pivot_longer(cols = -h_step, names_to = "Method", values_to = "MAPE")

## Add columns for line type and method
mape_data_long <- mape_data_long %>%
  mutate(
    LineType = case_when(
      grepl("rec", Method) ~ "Reconciled",
      grepl("temp", Method) ~ "Temporal Reconciled",
      TRUE ~ "Base"
    ),
    MethodGroup = case_when(
      grepl("arima", Method) ~ "ARIMA",
      grepl("vecm", Method) ~ "VECM",
      Method == "mape_DTF" ~ "DTF"
    )
  )

## Create the plot with specified colors and line types
ggplot_mape <- ggplot(mape_data_long, aes(x = h_step, y = MAPE, colour = MethodGroup, linetype = LineType)) +
  geom_line(linewidth = 1.1) +
  scale_color_manual(values = c(
    "ARIMA" = "blue",
    "VECM" = "red",
    "DTF" = "green"
  )) +
  scale_linetype_manual(values = c(
    "Base" = "solid",
    "Reconciled" = "dashed",
    "Temporal Reconciled" = "dotted"
  )) +
  labs(x = "h-step Forecast", y = "MAPE Value", title = "Monthly total LTD base and reconciled forecast MAPE by Method") +
  scale_x_continuous(breaks = 1:12) +
  theme_minimal() +
  guides(
    colour = guide_legend(title = "Colour: Model", override.aes = list(linetype = "solid")),
    linetype = guide_legend(title = "Line type: Reconciled/Base")
  )

ggplot_mape


```



## Comparing against DTF forecast

# Discussion
## Interpretation of Results
Analysis of what this forecasts mean for DTF.

## Limitations and Assumptions
Any limitations encountered during the forecasting process.

# Conclusion and Recommendations
## Summary 
Recap the findings and their implications.

## Future Work
Suggestions for improving future forecasts.

# Appendix


```{r}
#| label: tbl-sumstat
#| tbl-cap: "Summary Statistics of variables"
#| echo: False
#| message: false
#| warning: false
#| 
# Create an empty dataframe to store results
numerical_summary <- data.frame(
  Variable = character(0),
  Min = numeric(0),
  Quartile_1 = numeric(0),
  Median = numeric(0),
  Mean = numeric(0),
  Quartile_3 = numeric(0),
  Max = numeric(0),
  stringsAsFactors = FALSE
)

options(scipen = 999)

# Create summary statistics table for numerical variables
for (i in 2:ncol(ltd_unit_ts)) {
  x <- ltd_unit_ts[, i]
  
  # Calculate summary statistics using summary()
  summary_result <- summary(x)
  
  # Define a regular expression pattern to match the numeric value
  pattern <- "-?\\d+\\.?\\d*"
  
  # Create a data frame for the current numerical variable
  result_df <- data.frame(
    Variable = names(x),
    Min = format(as.numeric(regmatches(summary_result[1], gregexpr(pattern, summary_result[1], perl=TRUE))[[1]]), big.mark = ","),
    Quartile_1 = format(as.numeric(regmatches(summary_result[2], gregexpr(pattern, summary_result[2],perl=TRUE))[[1]][2]), big.mark = ","),
    Median = format(as.numeric(regmatches(summary_result[3], gregexpr(pattern, summary_result[3], perl=TRUE))[[1]]), big.mark = ","),
    Mean = format(as.numeric(regmatches(summary_result[4], gregexpr(pattern, summary_result[4], perl=TRUE))[[1]]), big.mark = ","),
    Quartile_3 = format(as.numeric(regmatches(summary_result[5], gregexpr(pattern, summary_result[5],perl=TRUE))[[1]][2]), big.mark = ","),
    Max = format(as.numeric(regmatches(summary_result[6], gregexpr(pattern, summary_result[6], perl=TRUE))[[1]]), big.mark = ","),
    stringsAsFactors = FALSE
  )
  
  # Bind the result to the summary dataframe
  numerical_summary <- rbind(numerical_summary, result_df)
}

# Set row names to be the names of the numerical variables
rownames(numerical_summary) <- numerical_summary$Variable
numerical_summary <- numerical_summary[,-1]

# Creating a scrollable HTML table
kable(numerical_summary, booktabs = TRUE)
```



# References
## Bibliography: 
Cite all data sources, literature, and software used in the report the insights generated by your work.

Hyndman, R.J., & Athanasopoulos, G. (2021) Forecasting: principles and practice, 3rd edition, OTexts: Melbourne, Australia. OTexts.com/fpp3. Accessed on <current date>.


https://treasury.gov.au/sites/default/files/2022-03/258735_real_estate_institute_of_australia.pdf

https://www.treasury.nsw.gov.au/sites/default/files/2021-06/the_economic_costs_of_transfer_duty_a_literature_review.pdf

