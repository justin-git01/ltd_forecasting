---
title: "Forecast Reconciliation for Land Transfer Duty"
author:
- name: Hoang Do
email: vdoo0002@student.monash.edu
organization: The Department of Treasury and Finance
bibliography: references.bib
format: report-pdf
output:
  monash::report:
    fig_caption: yes
    fig_height: 3
    fig_width: 4
    includes:
      in_header: preamble.tex
    keep_tex: yes
    number_sections: yes
    citation_package: biblatex
    toc: true
    extra_dependencies: ["float"]
---


```{r, warning=F, message= F, include=F}
# Load libraries
library(tidyverse)
library(readxl)
library(fpp3)
library(urca)
library(plotly)
library(knitr)
library(here)
library(tseries)
library(ggplot2)
library(kableExtra)

# Load rmse RDA file
load(here::here("data/rmse_base_vecm.RData"))
load(here::here("data/rmse_cross_temp_vecm.RData"))
load(here::here("data/rmse_temp_vecm.RData"))
load(here::here("data/rmse_cross_sec_vecm.RData"))

# Load mape RDA file
load(here::here("data/mape_base_vecm.RData"))
load(here::here("data/mape_cross_temp_vecm.RData"))
load(here::here("data/mape_temp_vecm.RData"))
load(here::here("data/mape_cross_sec_vecm.RData"))

```

```{r, warning=F, message= F}
# Load ltd aggregate date
ltd_agg <- read_excel(here("data/LTD_new.xlsx"), sheet = 1) |>
  rename(Date = ...1,
         ltd = LTD,
         sales = SALES,
         hvi = HVI,
         lending = LENDING) |>
  dplyr::select(c(Date, ltd, sales, hvi, lending))

# Load ltd unit data and join with aggregate data
ltd_unit <- read_excel(here("data/LTD_new.xlsx"), sheet = 2) |>
  rename(Date = ...1) |>
  dplyr::select(Date, ltd_total, ltd_nonres, ltd_comm, ltd_ind, ltd_other, ltd_res) |>
  left_join(ltd_agg, by = c("Date")) |>
  dplyr::select(-ltd)
```


```{r, warning=F, message= F}
# Tax revenue data 
tax_rev <- read_excel(here::here("data/tax_rev.xlsx"), sheet = 2) |>
  rename(Date = ...1)|>
  mutate(reliance = `Reliance on stamp duty` *100 ) |>
  dplyr::select(Date, reliance) |>
  mutate(Quarter = yearquarter(Date)) |>
  dplyr::select(-Date)  |>
  as_tsibble(index = Quarter) |>
  relocate(Quarter)
```

```{r}
dtf_rmse <- read_excel(here("data/final_dtf_rmse.xlsx"), sheet = 4)
dtf_mape <- read_excel(here("data/final_dtf_rmse.xlsx"), sheet = 5)
```

# Abstract

Accurate land transfer duty (LTD) forecasting is essential for effective government market monitoring and policy implementation. In this paper, we present a novel forecasting methodology for LTD in Victoria, with a particular focus on forecast reconciliation techniques. Given that time series data can often be disaggregated by various attributes of interest, and that forecasts are needed for both disaggregated and aggregated series, it is crucial that the forecasts align accurately with the data’s aggregation structure. Our methodology addresses this challenge by ensuring coherence across the entire aggregation hierarchy. Utilizing the Vector Error Correction Model (VECM) within cross-sectional, temporal, and cross-temporal hierarchical structures, we demonstrate that our approach significantly improves the forecast accuracy of Victoria’s LTD, using the *Department of Treasury and Finance*’s forecasts as a benchmark. Furthermore, this methodology is broadly applicable to other aggregate variables that can be modeled from disaggregated components and/or at different frequencies.

# Introduction and background

The property sector plays a pivotal role in Australia's economy, indirectly accounting for 1 in 4 jobs and contributing around 13% of Gross Domestic Product (GDP). In the 2021 financial year, property sales totaled approximately $350 billion, as reported by the @reia_2022. Land transfer duty, previously known as stamp duty, has a significant impact on property transactions and the sector as a whole. A study published by the New South Wales Treasury found that a 100 basis point (1%) cut in land transfer duty could boost property transactions by 10% (@nsw_treasury_2021).

Land transfer duty is a tax applied to the "dutiable value" of a property being purchased or acquired, whether it is a first home or an investment property. The dutiable value is determined as either the property's purchase price or its market value, whichever is greater. Several factors influence the amount of duty paid, including the buyer's intended use of the property, foreign purchaser status, and eligibility for exemptions.

```{r}
#| message: False
#| warning: False
#| echo: False
#| label: fig-taxrevenue
#| fig-cap: "Percentage of Victoria's Tax Revenue from Land Transfer Duty"
#| fig-align: 'center'

autoplot(tax_rev, reliance, color = "blue") +
  geom_hline(aes(yintercept = mean(reliance), linetype = "Average percentage"), color = "red", linewidth = 1.2) +
  scale_linetype_manual(name ="", values = c('dashed')) +
  labs(x = "Date", y = "Percentage", title = " ") +
  theme_bw()
```

Furthermore, Victoria’s tax revenue heavily relies on land transfer duty. As illustrated in @fig-taxrevenue, over the past 20 years, on average, land transfer duty has accounted for an average of **27%** of Victoria's tax revenue. Despite its perceived inequity and various exemptions designed to aid homebuyers, abolishing this duty remains challenging due to the need for equivalent revenue replacement. If this duty were removed, the government would need to introduce one or more new taxes to generate equivalent revenue.

To balance the need for sufficient tax income without discouraging property transactions, *the Department of Treasury and Finance* (**DTF**) supports the provision of better advice to the government on policy adjustments, subsidies, exemptions, and restrictions by delivering evidence-based insights. Accurate forecasts of land transfer duty, both short-term (*1 to 3 months*) and long-term (*12 months*), are essential for these decisions. 

In response, this paper introduces a new forecasting methodology aimed at improving the accuracy of these predictions. By employing forecast reconciliation and combining cross-sectional and temporal hierarchies, we aim to provide the *DTF* with more reliable forecasts to inform policy-making. The primary advantage of this methodology lies in its ability to produce forecasts at various aggregation levels within both cross-sectional and temporal hierarchies, thus accommodating the unique characteristics of different nodes. Our results use monthly land transfer duty data, covering the period from July 2013 to June 2024.

We employ three forecasting models, namely vector error correction model (VECM), vector autoregressive model (VAR) and seasonal naïve (Snaïve). For the model fitting process, we incorporate three macroeconomic indicators and property market indices, which are sales, home value index and lending sourced from the Australian Bureau of Statistics (ABS) and CoreLogic. The rationale for selecting these three explanatory variables is that they are also employed by the best-performing models used by the *DTF*.

To evaluate forecast performance, we use time series cross-validation, ensuring that the corresponding training set consists only of observations that occurred before the observation that forms the test set. The initial training set contains 111 months of data and is subsequently increased by one month for each successive training set. Consequently, there will be 10 testing sets with a 1 to 12-step-ahead forecast. We then compute accuracy measures, specifically root mean squared errors (RMSE) and mean absolute percentage errors (MAPE), for all forecasts, which are then averaged across the 10 sets.

Overall, the results suggest that reconciled forecasts, particularly when using cross-temporal reconciliation, consistently outperform base forecasts. Among the hierarchical structures, temporal hierarchies contribute more to accuracy improvements than cross-sectional hierarchies. The reconciled forecasts from VECM exhibit lower RMSE and MAPE compared to the *DTF*’s forecasts, highlighting the effectiveness of the proposed approach.

The remainder of the paper is organized as follows: Section 3 includes exploratory data analysis (EDA). Section 4 covers the forecasting methods in detail. Section 5 discusses the detailed time series cross-validation process adopted for this study. Section 6 presents the main results. Section 7 interprets the results and discusses limitations, and Section 8 concludes the study.

# Exploratory Data Analysis (EDA)

In this section, we conduct a detailed Exploratory Data Analysis (EDA) for the total land transfer duty and all its disaggregated levels, as well as for other included exploratory variables. Each of the three exploratory variables serves as a measure of property market performance and is expected to have interrelations with land transfer duty.

## Time series analysis {#sec-tsanalysis} 

```{r, message = FALSE, warning= FALSE}
ltd_agg_ts <- ltd_agg |>
  mutate(Month = yearmonth(Date)) |>
  select(-Date) |>
  as_tsibble(index = Month) |>
  relocate(Month)

ltd_unit_ts <- ltd_unit %>%
  mutate(Month = yearmonth(Date)) %>%
  select(-Date) %>%
  as_tsibble(index = Month) %>%
  relocate(Month)
```

For this study, the total land transfer duty is divided into residential and non-residential categories. The non-residential land transfer duty is further disaggregated into commercial, industrial, and other categories. @fig-crosssec1 provides an overview of the cross-sectional hierarchical structure.

![Cross-sectional hierarchy](image/cross_sec.png){#fig-crosssec1 fig-alt="Cross-sectional hierarchy" fig-align="center" width=65%} 


### Total Land Transfer Duty 

```{r}
#| message: False
#| warning: False
#| echo: False
#| label: fig-trend
#| fig-cap: "Time plot of Land Transfer Duty in Victoria over time"
#| fig-align: 'center'
#| fig-height: 3.5
#| fig-width: 4.5

ltd_unit_ts |>
  mutate(ltd_tot_inM = ltd_total/1000000) |>
autoplot(ltd_tot_inM, color = "red") +
  labs(title = " ", x = "Date", y = "Amount (in $m)") +
  theme_bw()
```

@fig-trend demonstrates an upward, non-linear trend, showing that the value of land transfer duty has generally increased over the ten-year period. However, due to the outbreak of Covid-19, total land transfer duty experienced a significant decline, reaching a low of 286.2 million in June 2020. Subsequently, it rebounded to an unprecedented peak of 1.013 billion in March 2022. This trend can be attributed to multiple rate cuts by the Reserve Bank of Australia, which initially aimed to stabilize the real estate sector and subsequently stimulated demand. Additionally, there is a noticeable increase in variability, particularly in recent years, indicating more pronounced fluctuations in the values. This increasing volatility suggests that a transformation of the data may be necessary to better analyze and model these trends.

```{r}
#| message: False
#| warning: False
#| echo: False
#| label: fig-sspattern
#| fig-cap: "Seasonal plot of Land Transfer Duty in Victoria"
#| fig-align: 'center'

ltd_unit_ts |>
  mutate(ltd_tot_inM = ltd_total/1000000) |>
  gg_season(ltd_tot_inM, labels = "both") +
  theme(legend.position = "none") +
  labs(x = "Date", y = "Amount (in $m)", title= " ")+
  theme_bw()
```

Considering the seasonal patterns of land transfer duty, referring to @fig-sspattern, it is clear that there is a large jump in land transfer duty in June and December each year. This may be attributed to June being the end of financial year, when buyers rush to complete transactions to take advantage of tax benefits, financial reporting, while December being the end of calender year, when many individuals aim to complete transactions before the holiday season. @fig-sspattern also shows that there is a decrease in April each year. 
```{r}
#| message: False
#| warning: False
#| echo: False
#| label: fig-dcmp
#| fig-cap: "Log of total Land Transfer Duty in Victoria (top) and its three additive components."
#| fig-align: 'center'
#| fig-height: 3.5
#| fig-width: 4.5
#| 
log_ltd_ts <- ltd_unit_ts |>
  mutate(log_ltd = log(ltd_total))

dcmp <- log_ltd_ts |>
  model(stl = STL(log_ltd))

## Plot all components
components(dcmp) |>
  autoplot() +
  theme_bw()
```

```{r}
#| message: False
#| warning: False
#| echo: False
#| label: fig-ssadj
#| fig-cap: "Seasonally adjusted of log Land Transfer Duty (blue) and the original data (grey)"
#| fig-align: 'center'
#| fig-height: 3.5
#| fig-width: 4.5
components(dcmp) |>
  as_tsibble() |>
  autoplot(log_ltd, colour = "gray") +
  geom_line(aes(y=season_adjust), colour = "#0072B2") +
  labs(x = "Date", y = "Amount", title = " ") +
  theme_bw()
```

@fig-dcmp shows that there is a seasonal pattern for log transformed land transfer duty, which agrees with what indicated from @fig-sspattern. The grey bars positioned to the left of each panel illustrate the relative scales of the components. The prominent grey bar in the seasonal panel indicates that the variation in the seasonal component is the smallest when compared to the overall variation in the data.

Moreover, @fig-dcmp demonstrates that after applying a log transformation, the variability becomes more consistent. However, the relatively small grey bar to the left of the remainder panel indicates that some heterogeneity persists.

The grey line in @fig-ssadj represents the log-transformed total land transfer duty, while the blue line represents the seasonally adjusted log-transformed total land transfer duty. @fig-ssadj suggests that by adjusting for the seasonal pattern, some local peaks and troughs have been smoothed out.

### Residential property land transfer duty
```{r}
#| message: False
#| warning: False
#| echo: False
#| label: fig-restrend
#| fig-cap: "Time plot of Residential property Land Transfer Duty in Victoria"
#| fig-align: 'center'
#| fig-height: 3.5
#| fig-width: 4.5

ltd_unit_ts |>
  mutate(ltd_res_inM = ltd_res/1000000) |>
autoplot(ltd_res_inM, color = "red") +
  labs(title = " ", x = "Date", y = "Amount (in $m)") +
  theme_bw()

```

@fig-restrend illustrates a non-linear upward trend in residential property land transfer duty, closely mirroring the overall trend in total land transfer duty. The figure also reflects the impact of the Covid-19 pandemic, with a significant decline to a low of 238.1 million in June 2020, followed by a rebound to an all-time high of 761.4 million in December 2021. This pattern is attributable to the fact that residential property comprises the largest portion of land transfer duty. These findings suggest that residential property remains a crucial component of the market, likely reflecting trends in housing demand and price fluctuations. Additionally, there is an increase in variability over time, which also suggests a transformation of the data.

```{r}
#| message: False
#| warning: False
#| echo: False
#| label: fig-resspattern
#| fig-cap: "Seasonal plot of Residential property Land Transfer Duty in Victoria"
#| fig-align: 'center'

ltd_unit_ts |>
  mutate(ltd_res_inM = ltd_res/1000000) |>
  gg_season(ltd_res_inM, labels = "both") +
  theme(legend.position = "none") +
  labs(x = "Date", y = "Amount (in $m)", title= " ")+
  theme_bw()
```

Since as discussed above, we expect residential land transfer duty also mirrors the seasonal pattern of total land transfer duty, which is shown in @fig-resspattern. There is also a large jump in land transfer duty in June and December each year, which can be explained by the same reason.

### Non-residential property land transfer duty
```{r}
#| message: False
#| warning: False
#| echo: False
#| label: fig-nonrestrend
#| fig-cap: "Time plot of Non-residential property Land Transfer Duty in Victoria"
#| fig-align: 'center'
#| fig-height: 3.5
#| fig-width: 4.5

ltd_unit_ts |>
  mutate(ltd_nonres_inM = ltd_nonres/1000000) |>
autoplot(ltd_nonres_inM, color = "red") +
  labs(title = " ", x = "Date", y = "Amount (in $m)") +
  theme_bw()

```

@fig-nonrestrend shows a markedly different pattern in non-residential property land transfer duty compared to total and residential property duty. While non-residential duty reached an all-time high of 357.2 million in March 2022 post-Covid-19, it lacks a consistent long-term growth trend, exhibiting significant variability. Moreover, the sudden drop after reaching its all time high in March 2022 may suggest that non-residential sector reacts more significantly to changes in the cash rate than the residential sector. 

```{r}
#| message: False
#| warning: False
#| echo: False
#| label: fig-nonresspattern
#| fig-cap: "Seasonal plot of Non-residential property Land Transfer Duty in Victoria"
#| fig-align: 'center'

ltd_unit_ts |>
  mutate(ltd_nonres_inM = ltd_nonres/1000000) |>
  gg_season(ltd_nonres_inM, labels = "both") +
  theme(legend.position = "none") +
  labs(x = "Date", y = "Amount (in $m)", title= " ")+
  theme_bw()
```

@fig-nonresspattern also shows an increase in June and December each year. Although the magnitude of change is smaller compared to total and residential property land transfer duty, the increasing pattern is more consistent across all years.

### Commercial property land transfer duty
```{r}
#| message: False
#| warning: False
#| echo: False
#| label: fig-commtrend
#| fig-cap: "Time plot of Commercial property Land Transfer Duty in Victoria"
#| fig-align: 'center'
#| fig-height: 3.5
#| fig-width: 4.5

ltd_unit_ts |>
  mutate(ltd_comm_inM = ltd_comm/1000000) |>
autoplot(ltd_comm_inM, color = "red") +
  labs(title = " ", x = "Date", y = "Amount (in $m)") +
  theme_bw()

```

@fig-commtrend demonstrates the fluctuations in commercial property land transfer duty from 2014 to 2024. The plot also exhibits significant variability, with several peaks and troughs over the observed period, ranging from 22.1 million to 132.4 million. Notably, the commercial land transfer duty reached its highest levels in early 2022, reflecting a post-Covid-19 recovery. However, unlike the residential sector, the commercial property sector does not show a consistent long-term growth trend. 

```{r}
#| message: False
#| warning: False
#| echo: False
#| label: fig-commspattern
#| fig-cap: "Seasonal plot of Commerical property Land Transfer Duty in Victoria"
#| fig-align: 'center'

ltd_unit_ts |>
  mutate(ltd_comm_inM = ltd_comm/1000000) |>
  gg_season(ltd_comm_inM, labels = "both") +
  theme(legend.position = "none") +
  labs(x = "Date", y = "Amount (in $m)", title= " ")+
  theme_bw()
```

@fig-commspattern demonstrates various increases throughout the year, in March, May, June and December, highlighting significant fluctuations. Moreover, while some years display consistent seasonal patterns, others exhibit irregular fluctuations.

### Industrial property land transfer duty
```{r}
#| message: False
#| warning: False
#| echo: False
#| label: fig-indtrend
#| fig-cap: "Time plot of Industrial property Land Transfer Duty in Victoria"
#| fig-align: 'center'
#| fig-height: 3.5
#| fig-width: 4.5

ltd_unit_ts |>
  mutate(ltd_ind_inM = ltd_ind/1000000) |>
autoplot(ltd_ind_inM, color = "red") +
  labs(title = " ", x = "Date", y = "Amount (in $m)") +
  theme_bw()

```

@fig-indtrend shows a more stable variability compared to residential or total land transfer duty, with less pronounced fluctuations, except for notable peaks observed in mid-2018 and March 2022. Overall, the trend is non-increasing, indicating a lack of long-term growth. The industrial sector has collected land transfer duty ranging from 5.2 million and 214.4 million, highlighting some large-scale industrial transactions or developments during certain periods. These observations suggest that while industrial property transactions exhibit periodic spikes, they generally maintain a steady pattern without significant upward or downward trends.

```{r}
#| message: False
#| warning: False
#| echo: False
#| label: fig-indspattern
#| fig-cap: "Seasonal plot of Industrial property Land Transfer Duty in Victoria"
#| fig-align: 'center'

ltd_unit_ts |>
  mutate(ltd_ind_inM = ltd_ind/1000000) |>
  gg_season(ltd_ind_inM, labels = "both") +
  theme(legend.position = "none") +
  labs(x = "Date", y = "Amount (in $m)", title= " ")+
  theme_bw()
```

Apart from slight increases in August and December each year, @fig-indspattern shows no clear indication of a seasonal pattern.

### Other property land transfer duty
```{r}
#| message: False
#| warning: False
#| echo: False
#| label: fig-othertrend
#| fig-cap: "Time plot of Other type property Land Transfer Duty in Victoria"
#| fig-align: 'center'
#| fig-height: 3.5
#| fig-width: 4.5

ltd_unit_ts |>
  mutate(ltd_other_inM = ltd_other/1000000) |>
autoplot(ltd_other_inM, color = "red") +
  labs(title = " ", x = "Date", y = "Amount (in $m)") +
  theme_bw()

```

@fig-othertrend shows a non-linear increasing trend in other types of property land transfer duty, including agricultural properties, with varying levels of variability over time and irregular peaks. Contributions from these sectors range from 3.3 million to 74.5 million, indicating sporadic activity that may correspond with specific market or economic conditions.

```{r}
#| message: False
#| warning: False
#| echo: False
#| label: fig-otherspattern
#| fig-cap: "Seasonal plot of Other type property Land Transfer Duty in Victoria"
#| fig-align: 'center'

ltd_unit_ts |>
  mutate(ltd_other_inM = ltd_other/1000000) |>
  gg_season(ltd_other_inM, labels = "both") +
  theme(legend.position = "none") +
  labs(x = "Date", y = "Amount (in $m)", title= " ")+
  theme_bw()
```

As shown in @fig-otherspattern, the seasonal pattern is quite irregular across all years, but a consistent increase is observed in December for the past several years, starting from 2020.

### Sales

The sales variable indicates the number of properties sold in Victoria, measured in units at the contract date for all types of residential dwellings.

```{r}
#| message: False
#| warning: False
#| echo: False
#| label: fig-salestrend
#| fig-cap: "Time Plot of Units of Properties Sold in Victoria"
#| fig-align: 'center'
#| fig-height: 3.5
#| fig-width: 4.5
ltd_unit_ts |>
  autoplot(sales, color = "red") +
  labs(title = " ", x = "Date", y = "Units") +
  theme_bw()
```

@fig-salestrend shows a non-linear trend with strong fluctuations in sales units over the ten-year period from January 2014 to January 2024. The plot exhibits considerable variability, with both high and low peaks scattered throughout the timeline. The values range from approximately 5,000 to over 15,000, indicating significant changes in sales units.

In recent years, there is a noticeable increase in variability, particularly around 2020 and onwards. This increasing volatility indicates that a transformation of the data may be necessary to better analyze and model these trends.

```{r}
#| message: False
#| warning: False
#| echo: False
#| label: fig-salessspattern
#| fig-cap: "Seasonal plot of Units of Properties Sold in Victoria"
#| fig-align: 'center'

ltd_unit_ts |>
  gg_season(sales, labels = "both") +
  theme(legend.position = "none") +
  labs(x = "Date", y = "Amount", title= " ") +
  theme(
    axis.text.y = element_text(size = 10, hjust = 1),
    axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
    plot.title = element_text(hjust = 0.5),
    legend.position = "right"
  ) +
  theme_bw()
```

@fig-salessspattern exhibits a significant decline in sales units in December, followed by a trough in January each year. This trend contrasts with the observed increase in total land transfer duty in December, as depicted in the seasonal plot in @fig-sspattern. This discrepancy can be attributed to the fact that, despite the reduction in the number of sales units, the properties sold during this period are typically of higher value, thereby driving up the total land transfer duty.

```{r}
#| message: False
#| warning: False
#| echo: False
#| label: fig-salesdcmp
#| fig-cap: "Log of Units of Properties Sold in Victoria (Top) and Its Three Additive Components"
#| fig-align: 'center'
#| fig-height: 3.5
#| fig-width: 4.5
log_ltd_ts <- ltd_unit_ts |>
  mutate(log_sales = log(sales))

dcmp <- log_ltd_ts |>
  model(stl = STL(log_sales))

## Plot all components
components(dcmp) |>
  autoplot() +
  theme_bw()
```

```{r}
#| message: False
#| warning: False
#| echo: False
#| label: fig-salesssadj
#| fig-cap: "Seasonally adjusted of log of Units of Properties Sold in Victoria (blue) and the original data (grey). "
#| fig-align: 'center'
#| fig-height: 3.5
#| fig-width: 4.5
components(dcmp) |>
  as_tsibble() |>
  autoplot(log_sales, colour = "gray") +
  geom_line(aes(y=season_adjust), colour = "#0072B2") +
  labs(x = "Date", y = "Amount", title = " ") +
  theme_bw()
```

@fig-salesdcmp indicates a seasonal pattern for log-transformed sales, which aligns with @fig-salessspattern. However, the large grey bar in the seasonal panel indicates that the variation in the seasonal component is the smallest when compared to the overall variation in the data.

Moreover, @fig-salesdcmp demonstrates that after applying a log transformation, the variability becomes more consistent, although some heterogeneity still remains.

The grey line in @fig-salesssadj represents the original log-transformed sales, while the blue line represents the seasonally adjusted log-transformed sales. @fig-salesssadj suggests that there is a significant difference between seasonally adjusted sales and original sales, especially in periodic drops.


### Home Value Index

The Home Value Index aims to measure monthly movements in the value of Australian housing markets.

```{r}
#| message: False
#| warning: False
#| echo: False
#| label: fig-hvitrend
#| fig-cap: "Time plot of Home Value Index in Victoria over time"
#| fig-align: 'center'
#| fig-height: 3.5
#| fig-width: 4.5
ltd_unit_ts |>
  autoplot(hvi, color = "red") +
  labs(title = " ", x = "Date", y = "Index Point") +
  theme_bw()
```

@fig-hvitrend illustrates an increasing trend, with some considerable variability throughout the timeline. There is no clear sign of seasonal pattern in home value index.

### Lending

Lending is the new borrower-accepted finance commitments for housing, personal and business loans from Australian Bureau Statistics (ABS), relating to all types of residential transactions.

```{r}
#| message: False
#| warning: False
#| echo: False
#| label: fig-lendingtrend
#| fig-cap: "Time plot of Lending in Victoria over time"
#| fig-align: 'center'
#| fig-height: 3.5
#| fig-width: 4.5
ltd_unit_ts |>
  autoplot(lending, color = "red") +
  labs(title = " ", x = "Date", y = "Value (in $m)") +
  theme_bw()
```

@fig-lendingtrend shows an overall increasing trend over time with clear seasonal pattern for lending.

## Dealing with seasonality {#sec-deal}

As discussed in @sec-tsanalysis, many time series clearly exhibit seasonal patterns. Given the limitations of Vector Error Correction Models (VECM) in adequately modeling these seasonal fluctuations, we propose an adjustment. Specifically, we will decompose the time series into seasonally adjusted components and explicit seasonal factors. This approach allows us to apply VECM to the seasonally adjusted data. Subsequently, we will conduct independent forecasts for both the seasonally adjusted data and the seasonal components. The final step involves reconciling these forecasts and then integrating them.

## Cointegration Analysis for modelling {#sec-coint}

Based on the observations from @fig-trend, which illustrates the trend and increasing variance pattern in land transfer duty while other disaggregated levels of land transfer duty exhibit either a trend or seasonal pattern, we can conclude that these series are non-stationary. The same findings apply to the three exploratory variables.

Furthermore, to justify the use of the Vector Error Correction Model (VECM) by the *Department of Treasury and Finance*, it is essential to test for the presence of cointegration patterns (or long-term equilibrium relationships) among these three series.

The Johansen procedure with the trace test statistic will be employed to test for cointegration between log transformed of land transfer duty, lending, and sales trend. This procedure tests the null hypothesis that no cointegration relationship exists among the series.

```{r}
# Rename ltd unit data
names(ltd_unit) <- c("Date", "Total", "NonRes", "Comm", "Ind", "Other", "Res", "Sales", "hvi", "lending")

# Convert ltd unit data to tsibble object
ltd_unit <- ltd_unit %>%
  mutate(Month = yearmonth(Date)) %>%
  select(-Date) %>%
  as_tsibble(index = Month) %>%
  relocate(Month)

# Filter out lending as it does not require seasonally adjusted
lend <- ltd_unit |>
  select(lending) |>
  relocate(Month)

# Total
## Decomp
dcmp <- ltd_unit |> 
  model(stl = STL(Total))

## Filter out SA
total_ltd_sa <- components(dcmp) |>
  select(season_adjust) |>
  relocate(Month) |>
  rename(Total = season_adjust)

## Filter out seasonality
total_ltd_s <- components(dcmp) |>
  select(season_year) |>
  relocate(Month) |>
  rename(Total = season_year)

# Residential
## Decomp
dcmp <- ltd_unit |> 
  model(stl = STL(Res))

## Filter out SA
res_ltd_sa <- components(dcmp) |>
  select(season_adjust) |>
  relocate(Month) |>
  rename(Res = season_adjust)

## Filter out seasonality
res_ltd_s <- components(dcmp) |>
  select(season_year) |>
  relocate(Month) |>
  rename(Res = season_year)

# NonRes
## Decomp
dcmp <- ltd_unit |> 
  model(stl = STL(NonRes))

## Filter out SA
nonres_ltd_sa <- components(dcmp) |>
  select(season_adjust) |>
  relocate(Month) |>
  rename(NonRes = season_adjust)

## Filter out seasonality
nonres_ltd_s <- components(dcmp) |>
  select(season_year) |>
  relocate(Month) |>
  rename(NonRes = season_year)

# Comm
## Decomp
dcmp <- ltd_unit |> 
  model(stl = STL(Comm))

## Filter out SA
comm_ltd_sa <- components(dcmp) |>
  select(season_adjust) |>
  relocate(Month) |>
  rename(Comm = season_adjust)

## Filter out seasonality
comm_ltd_s <- components(dcmp) |>
  select(season_year) |>
  relocate(Month) |>
  rename(Comm = season_year)

# Ind
## Decomp
dcmp <- ltd_unit |> 
  model(stl = STL(Ind))

## Filter out SA
ind_ltd_sa <- components(dcmp) |>
  select(season_adjust) |>
  relocate(Month) |>
  rename(Ind = season_adjust)

## Filter out seasonality
ind_ltd_s <- components(dcmp) |>
  select(season_year) |>
  relocate(Month) |>
  rename(Ind = season_year)

# other
## Decomp
dcmp <- ltd_unit |> 
  model(stl = STL(Other))

## Filter out SA
other_ltd_sa <- components(dcmp) |>
  select(season_adjust) |>
  relocate(Month) |>
  rename(Other = season_adjust)

## Filter out seasonality
other_ltd_s <- components(dcmp) |>
  select(season_year) |>
  relocate(Month) |>
  rename(Other = season_year)

# Sales
# Decompositions
dcmp <- ltd_unit |>
  model(stl = STL(Sales))

# Filter out sales trend
sales_trend <- components(dcmp) |>
  select(trend) |>
  relocate(Month) |>
  rename(sales_trend = trend)

# Filter out SA sales
sales_sa <- components(dcmp) |>
  select(season_adjust) |>
  relocate(Month) |>
  rename(Sales = season_adjust)

# HVI
# Decompositions
dcmp <- ltd_unit |>
  model(stl = STL(hvi))

# Filter out SA hvi
hvi_sa <- components(dcmp) |>
  select(season_adjust) |>
  relocate(Month) |>
  rename(hvi = season_adjust)


# Seasonal adjusted data
sa_data <- total_ltd_sa |>
  left_join(nonres_ltd_sa, by = c("Month")) |>
  left_join(comm_ltd_sa, by = "Month") |>
  left_join(ind_ltd_sa, by = "Month") |>
  left_join(other_ltd_sa, by = "Month") |>
  left_join(res_ltd_sa, by = "Month") |>
  left_join(sales_sa, by = "Month") |>
  left_join(hvi_sa, by = "Month") |>
  left_join(lend, by = "Month") |>
  left_join(sales_trend, by = "Month")

# Seasonal data
s_data <- total_ltd_s |>
  left_join(nonres_ltd_s, by = c("Month")) |>
  left_join(comm_ltd_s, by = "Month") |>
  left_join(ind_ltd_s, by = "Month") |>
  left_join(other_ltd_s, by = "Month") |>
  left_join(res_ltd_s, by = "Month") 
```

```{r}
#| label: tbl-jotest
#| tbl-cap: "Johansen test result"
#| echo: False
#| message: false
#| warning: false
jtest <- ca.jo(log(sa_data[,c(2,10,11)]), type="trace", K=4, ecdet="none", spec="longrun")

jtest_sum <- summary(jtest)

# Extract Eigenvalues
eigenvalues <- round(jtest_sum@lambda, 2)

# Extract Test Statistics
test_statistics <- round(jtest_sum@teststat,2)
critical_values <- jtest_sum@cval

# Extract Eigenvectors
eigenvectors <- round(jtest_sum@V,2)

# Extract Weights
weights <- round(jtest_sum@W,2)

# Format Test Statistics and Critical Values
test_stats_table <- data.frame(
  Rank = c("r <= 2", "r <= 1", "r = 0"),
  Test_Statistic = test_statistics,
  `10%` = critical_values[,1],
  `5%` = critical_values[,2],
  `1%` = critical_values[,3]
)

# Format Eigenvectors
eigenvectors_table <- data.frame(
  Variable = rownames(eigenvectors),
  eigenvectors
)

# Format Weights
weights_table <- data.frame(
  Variable = rownames(weights),
  weights
)

# Display Eigenvalues
# kable(matrix(eigenvalues, ncol = 1), col.names = "Eigenvalue", label = "Eigenvalues (lambda)") %>%
#   kable_styling()

# Display Test Statistics
kable(test_stats_table, col.names = c("Rank", "Test Statistic", "10%", "5%", "1%"), label = "Values of test statistics and critical values of test") %>%
  kable_styling()

# Display Eigenvectors
# kable(eigenvectors_table, col.names = c("Variable", "ltd_total.l3", "sales.l3", "hvi.l3"), label = "Eigenvectors, normalised to the first column") %>%
#   kable_styling()

# Display Weights
# kable(weights_table, col.names = c("Variable", "ltd_total.l3", "sales.l3", "hvi.l3"), label = "Weights W (Loading Matrix)") %>%
#   kable_styling()

```

The test results, as shown in @tbl-jotest, using four lags as chosen by the DTF, allow us to reject the null hypothesis that $r = 1$, yet we fail to reject the null hypothesis that $r \leq 1$ at 5% level of significance. This suggests that among the three variables, the rank of the matrix equals to 1, indicating the presence of one cointegration relationships.

We will now construct a linear combination to re-verify if there is cointegration relationship between these 3 variables or confirm the validity of the use of VECM. To create this linear combination, we can utilize the components of the eigenvector associated with the largest eigenvalue. According to the Johansen test summary, the largest eigenvalue is approximately `r round(jtest_sum@lambda[1],2)`. This corresponds to the eigenvectors components of `ltd_total`, which is approximately equal to (`r round(jtest_sum@V[,1],2)`). By forming a linear combination of the series using these eigenvector components, we can achieve a stationary series.

```{r}
#| message: False
#| warning: False
#| echo: False
#| label: fig-splot
#| fig-cap: "Stationary series formed via a linear combination of 3 time series"
#| fig-align: 'center'
#| fig-height: 3
#| fig-width: 4

eigenvectors <- as.numeric(jtest_sum@V[,1])

s <- sa_data |>
mutate(log_ltd = log(Total), 
       log_sales = log(lending),
       log_hvi = log(sales_trend)) |> 
dplyr::select(c(log_ltd, log_sales, log_hvi)) |> 
summarise(s1 = log_ltd*eigenvectors[1] + eigenvectors[2]*log_sales + eigenvectors[3]*log_hvi) 

df <- data.frame(Date = s$Month, Value = as.numeric(unlist(s[,2])))

ggplot(df, aes(x = Date, y = Value)) + 
  geom_line(color = "red") + 
  labs(y = "Value", x = "Date", title = " ") +
theme_bw()

```

As shown in @fig-splot, the linear combination appears to be more stationary, although there remains a slight indication of varying variances over time.

```{r}
#| echo: False
#| message: false
#| warning: false

linear_comb <- as.numeric(unlist(s[,2]))

adf_result <- adf.test(linear_comb)

# Extract ADF Test components
adf_statistic <- round(adf_result$statistic,2)
lag_order <- round(adf_result$parameter,1)
p_value <- round(adf_result$p.value,2)

# Format ADF Test result into a table
adf_table <- data.frame(
  Metric = c("Dickey-Fuller", "Lag Order", "p-value"),
  Value = c(adf_statistic, lag_order, p_value)
)

# Display ADF Test result
# kable(adf_table) %>%
#   kable_styling()

```

However, the p-value of `r p_value` from the Augmented Dickey-Fuller (ADF) test indicates that we cannot reject the null hypothesis of a unit root, providing evidence of a non-stationary series formed from the linear combination, which disproves the validity of the use of VECM.

Since we will fit VECM to all the variables in the hierarchy, including residential, non-residential, commercial, property and other, we will perform the same procedure of cointegration analysis as above.

```{r}
#| label: tbl-adftest
#| tbl-cap: "Johansen and ADF test for other type ltd"
#| echo: False
#| message: false
#| warning: false
#| 
# Assuming the data is already loaded in ltd_unit_ts
variables <- c("Res", "NonRes", "Comm", "Ind", "Other")
p_values <- data.frame(Variable = character(), Johansen_p_value = numeric(), ADF_p_value = numeric(), stringsAsFactors = FALSE)

for (var in variables) {
  # Johansen Test
  jtest <- ca.jo(log(sa_data[,c(var, "lending", "sales_trend")]), type="trace", K=4, ecdet="none", spec="longrun")
  jtest_sum <- summary(jtest)
  johansen_p_value <- sum(jtest_sum@teststat > jtest_sum@cval[,2])

  eigenvectors <- as.numeric(jtest_sum@V[,1])

  s <- sa_data |>
  mutate(log_ltd = log(!!sym(var)), 
         log_sales = log(lending),
         log_hvi = log(sales_trend)) |> 
  dplyr::select(c(log_ltd, log_sales, log_hvi)) |> 
  summarise(s1 = log_ltd*eigenvectors[1] + 
              eigenvectors[2]*log_sales + 
              eigenvectors[3]*log_hvi) 

  # ADF Test
  linear_comb <- as.numeric(unlist(s[, 2]))
  adf_result <- adf.test(linear_comb)
  adf_p_value <- round(adf_result$p.value,2)
  
  # Append results to the data frame
  p_values <- rbind(p_values, data.frame(Variable = var, cointegration = johansen_p_value, ADF_p_value = adf_p_value))
}

# Display the results in a table
kable(p_values, col.names = c("Variable", "Number of cointegrating relationship", "ADF p-value")) %>%
  kable_styling()

```

As presented in @tbl-adftest, the ADF test results indicate that the null hypothesis of non-stationarity for the linear combination cannot be rejected for residential LTD, whereas the linear combination for other types of LTD is stationary.

Given that the null hypothesis of non-stationarity cannot be rejected for both total LTD and residential LTD, we opted to fit the model using sales and HVI instead. Following the same procedure, the ADF test results show that the null hypothesis can be rejected at the 10% significance level for residential LTD but not for total LTD. Consequently, we decided to proceed with fitting the VECM model for residential LTD using sales and HVI.

For total LTD, despite the ADF test indicating non-stationarity in the linear combination for both model, we decide to proceed with the VECM model because the Johansen test still identifies a cointegrating relationship. This suggests that, while the linear combination may be non-stationary, the VECM can still effectively model the underlying long-term equilibrium and adjust the forecasts accordingly, making it a suitable choice for this analysis.

# Methodology
## Cross-sectional, temporal and cross-temporal hierarchies

Upon analyzing the data characteristics and the disaggregation of LTD, it becomes evident that a three-level hierarchical structure can be established. Generating forecasts at various levels of aggregation is valuable, driven by diverse reasons and objectives. For example, each level of aggregation may exhibit distinct characteristics; for instance, transactions involving residential properties might vary from those involving non-residential properties due to differences in market dynamics or market size for each property type.

In this context, total land transfer duty is divided into two categories: residential and non-residential properties. Non-residential properties are further disaggregated into three sub-categories: commercial, industrial, and other, which predominantly includes agricultural properties. @fig-crosssec illustrates the cross-sectional hierarchical structure of land transfer duty as discussed.

![Cross-sectional hierarchy](image/cross_sec.png){#fig-crosssec fig-alt="Cross-sectional hierarchy" fig-align="center" width=65%} 

Given that this is a straightforward cross-sectional hierarchical structure, we will also consider temporal hierarchies to further enhance forecast accuracy. Land transfer duty is collected monthly, and forecasts can be generated at bi-monthly, quarterly, four-monthly, semi-annual, and annual frequencies. Various temporal hierarchies can be constructed with monthly land transfer duty treated as the bottom level. @fig-temp presents an example of temporal hierarchies with monthly land transfer duty as the bottom level.

![Temporal hierarchy](image/temp.png){#fig-temp fig-align="center"}

Although forecasts using cross-sectional and temporal hierarchies have demonstrated substantial improvements (@kourentzes2019cross), these approaches have typically been used separately. By combining cross-sectional hierarchies and temporal hierarchies, referred to as cross-temporal hierarchies, forecast accuracy can be further improved. Moveover, another advantage of cross-temporal reconciled forecasts is that it provides aligned short-term and long-term decision. If cross-sectional or temporal forecasts are used independently, some outputs, such as very long-term forecasts at a very disaggregated level—for instance, two years of forecasts for commercial property land transfer duty—may not be directly useful. Therefore, one would have to post-process the forecasts further, for example combining together multiple long-term disaggregate bottom level forecasts (commerical, industrial, other) to produce long-term total land transfer duty forecasts, which would then break the desired coherence across all levels and time periods. @fig-crosstemp from @kourentzes2019cross shows an example of a cross-temporal hierarchical structure.

![Cross-temporal hierarchy](image/cross-temporal.png){#fig-crosstemp fig-align="center"}

## Forecast reconciliation

From the cross-temporal hierarchies, forecasts can be produced at all levels for every nodes. In an ideal scenario, forecasts from various levels of aggregation would sum up to match the top-level forecast. However, practical implementation often reveals incoherent among independently produced forecasts. Each independently produced forecast is subject to different errors, and the summation of these errors from different nodes can result in aggregated forecasts that deviate from the independently produced top-level forecast. Consequently, aligning forecasts and ensuring they aggregate correctly according to the hierarchical structure becomes critical. Forecast reconciliation has emerged as one of the most effective methodologies to address this challenge to date.

We first consider cross-sectional forecast reconciliation. Recall from @fig-crosssec, we can construct this hierarchical structure for LTD in a matrix form like this:

$$
\begin{bmatrix}
  \text{Total}_{t} \\
  \text{Non-residential}_{t} \\
  \text{Residential}_{t} \\
  \text{Commercial}_{t} \\
  \text{Industrial}_{t} \\
  \text{Other}_{t} \\
  \end{bmatrix}
=
\begin{bmatrix}
  1 & 1 & 1 & 1 \\
  0 & 1 & 1 & 1 \\
  1 & 0 & 0 & 0 \\
  0 & 1 & 0 & 0 \\
  0 & 0 & 1 & 0 \\
  0 & 0 & 0 & 1 \\
\end{bmatrix}
\begin{bmatrix}
  \text{Residential}_{t} \\
  \text{Commercial}_{t} \\
  \text{Industrial}_{t} \\
  \text{Other}_{t} \\
\end{bmatrix}
$$

or in a more compact notation:

$$
\textbf{y}_{t} = \textbf{S}\textbf{b}_{t},
$$

where $\textbf{y}_t$ is an $n$-dimensional vector of all the observations in the hierarchy at time $t$, $\bf{S}$ represents the summing matrix defining how bottom-level series are aggregated, and $\textbf{b}_t$ is an $m$-dimensional vector of all the observations in the bottom level of the hierarchy at time $t$. In this case, $n$ and $m$ equal to 6 and 4, respectively.

Then for any set of base forecast, denoted as $\bf{\hat{y}}_{h}$, where h is the forecast horizon, all reconciliation forecasting approaches, generating coherent forecasts $\bf{\tilde{y}}_{h}$, can be represented as:

$$
\bf{\tilde{y}}_{h} = \bf{SG}\hat{\bf{y}}_{h},
$$
where $\bf{G}$ is a matrix that maps the base forecasts into the bottom level (@hyndman2021forecasting).

The equation shows that pre-multiplying any set of base forecasts with $\bf{SG}$ will return a set of coherent forecasts.

Within the domain of hierarchical time series forecasting, there are three traditional single level approaches for generating forecasts for hierarchical time series. The first, known as the *bottom-up* approach, initiates by producing forecasts for each series at the lowest level and subsequently aggregates these to generate forecasts for the upper levels of the hierarchy. For this approach, $\bf{G}$ can be defined as:

$$
\textbf{G}
=
\begin{bmatrix}
  0 & 0 & 1 & 0 & 0 & 0 \\
  0 & 0 & 0 & 1 & 0 & 0 \\
  0 & 0 & 0 & 0 & 1 & 0 \\
  0 & 0 & 0 & 0 & 0 & 1 \\
\end{bmatrix},
$$
where the first 2 columns zero out the base forecast of the series above the bottom level. 

Conversely, the *top-down* approach starts with a forecast at the highest level, which is then disaggregated to lower levels using predetermined proportions—typically based on historical data distributions (@gross1990disaggregation). For this approach, $\bf{G}$ can be defined as:

$$
\textbf{G}
=
\begin{bmatrix}
  \text{p}_{1} & 0 & 0 & 0 & 0 & 0 \\
  \text{p}_{2} & 0 & 0 & 0 & 0 & 0 \\
  \text{p}_{3} & 0 & 0 & 0 & 0 & 0 \\
  \text{p}_{4} & 0 & 0 & 0 & 0 & 0 \\
\end{bmatrix},
$$
where the first column includes the set of proportions that distribute the base forecasts of the top level to the bottom level

Lastly, the *middle-out* approach is a combination of both the bottom-up and top-down methods. 

However, the traditional single level approaches may have their limitations since only base forecast at one level is used. In response to this, @wickramasuriya2018optimal introduced the *MinT* (Minimum Trace) optimal reconciliation methodology, which is based on the finding of a **G** matrix that minimises the total forecast variance within the set of coherent forecasts.

@wickramasuriya2018optimal show that the variance-covariance of the h-step-ahead *coherent forecast errors* is given by:
$$
\textbf{V}_{h} = Var[\textbf{y}_{T+h} - \bf{\tilde{y}}_\text{h}] = \textbf{SG}\textbf{W}_{h}\textbf{G'S'} ,
$$
where $\textbf{W}_{h} = Var[\textbf{y}_{T+h} - \bf{\hat{y}}_{h}]$ is the variance-covariance matrix of the corresponding *base forecast errors*.

@wickramasuriya2018optimal also show that: 
$$
\textbf{G} = (\textbf{S}'\textbf{W}_{h}^{-1}\textbf{S})^{-1}\textbf{S}'\textbf{W}_{h}^{-1},
$$
minimises the trace of $\textbf{V}_{h}$ subject to **S** **G** **S** = **S**

Therefore, the optimally reconciled forecasts are given by:
$$
\bf{\tilde{y}}_{h} = \textbf{S}(\textbf{S}'\textbf{W}_{h}^{-1}\textbf{S})^{-1}\textbf{S}'\textbf{W}_{h}^{-1}\bf{\hat{y}}_{h}
$$
which refers as the **MinT**

Now, a challenge with G is that it requires an estimation of $\textbf{W}_{h}$, the forecast error variance of h-step-ahead base forecasts. There are four simplifying approximations in place that have been shown to work well:

1. **OLS** (@hyndman2011optimal): 
    $\mathbf{W}_{h} = \text{k}_{h}\mathbf{I}$ for all h, where $\text{k}_{h} > 0$.

This is the simplest assumption to adopt, implying that $\bf{G}$ is independent of the data, which significantly reduces computational costs. However, this approach does not account for the varying scales across different levels of the structure or for the relationships between series.

2. $\bf{WLS}_{S}$  (@athanasopoulos2017forecasting):
    $\mathbf{W}_{h} = \text{k}_{h}\mathbf{\Lambda}$ for all h, where $\text{k}_{h} > 0$, $\mathbf{\Lambda} = diag(\textbf{S1})$, and $\bf{1}$ is  unit vector of dimension $\it{m}$ (the number of bottom-level series).
    
Applying structural scaling is particularly useful in cases where residuals are not available, and so the variance scaling cannot be applied; for example, in cases where the base forecasts are generated by judgemental forecasting.
    
3. $\bf{WLS}_{V}$ (@hyndman2016fast): 
    $\mathbf{W}_{h} = \text{k}_{h}\text{diag(}\mathbf{\hat{W}}_{1}\text{)}$ for all h, where $\text{k}_{h} > 0$,

$$
\mathbf{\hat{W}}_{1} = \frac{1}{T}\sum_{t=1}^{T}\textbf{e}_{t}\textbf{e'}_{t},
$$
and $\textbf{e}_{t}$ is an $\it{n}$-dimensional vector of residuals of the models that generated the base forecasts stacked in the same order as the data.

4. $\bf{MinT}_{S}$ (@wickramasuriya2018optimal): 
  $\mathbf{W}_{h} = \text{k}_{h}\mathbf{\hat{W}^{*}}_{1, D}$ for all h, where $\text{k}_{h} > 0$, and $\mathbf{\hat{W}^{*}}_{1, D} = \mathit{\lambda}\mathbf{\hat{W}^{*}}_{1, D} + \text{(1-} \mathit{\lambda})\mathbf{\hat{W}}_{1}$ is a shrinkage estimator with diagonal target $\mathbf{\hat{W}^{*}}_{1, D}$, a diagonal matrix comprising the diagonal entries of $\mathbf{\hat{W}}_{1}$, and $\mathit{\lambda}$ the shrinkage intensity parameter. 

In contrast to earlier estimators of variance and structural scaling, this method captures strong interrelationships between time series in the hierarchy, while the use of shrinkage helps to manage the complexity of the estimation due to the size of $\mathbf{W}_{h}$.
  
By following the above procedure, we will get cross-sectionally coherent forecasts. Moreover, this procedure extends to temporal reconciliation and cross-temporal reconciliation.
  
### Cross-sectional reconciliation
For cross-sectional reconciliation, there are two main challenges, as stated by @kourentzes2019cross: 

1. the size of the cross-sectional dimension of the hierarchy; and 
2. the heterogeneity of the series across, but also within levels. 

The size relates directly with estimation of $\mathbf{W}_{h}$, and therefore very large hierarchies, estimation of $\mathbf{W}_{h}$ can be computationally expensive. However, this is not the case for this hierarchical structure in @fig-crosssec. On the other hand, given that the time series across each level can represent very different entities, it is expected that there will be  heterogeneity between them. Therefore, in this case, we choose not to apply structural scaling since it assumes that each of the bottom-level base forecasts has errors with equal variance $\text{k}_{h}$. 

### Temporal reconciliation

On the other hand, for temporal hierarchies, forecasts across all levels are for the same series, therefore it is safe to assume homogeneity between each level. On the other hand, following the arguments by @athanasopoulos2017forecasting, since the covariances in $\mathbf{W}_{h}$ would be between series of different sampling frequencies due to the temporal aggregation, we do not implement the MinT shrinkage estimator.

### Cross-temporal reconciliation

The cross-temporal reconciliation methodology used in this context is heuristic first-temporal-then-cross-sectional reconciliation, proposed by @kourentzes2019cross. It is a 2-step approach, where step 1 includes reconciliation through temporal hierarchies for each single variable to achieve temporally coherence forecast (**THieFs**). Then for step 2, from previously **THieFs**, we generate k cross-sectional reconciliations, setting $\textbf{W}_{h} = \bf{\hat{W}}_{h,\ell}$, where $\ell = 1,2,..,k$, and k denotes the number of temporal aggregation levels. This results in reconciliation matrix $\textbf{SG}_{\ell}$ for each temporal aggregation level. And by averaging across these, we compose a consesus reconciliation matrix **SG**, where $\textbf{G} = \frac{1}{k}\sum_{\ell=1}^{k}\textbf{G}_{\ell}$, capturing the reconciliation consesus across all *k* temporal aggregation levels. The outcome are cross-temporally reconciled forecasts, which are coherent across both dimensions, at all scales. 

## Model Selection {#sec-model}

As discussed in @sec-deal and @sec-coint, the relevance of employing VECM to adequately model the seasonally adjusted time series is evident.

One limitation of this model is its requirement for a sufficiently large data size. Monthly data spanning over a decade for LTD has been found to be sufficient. However, since the sample size of annually aggregated frequencies is only 10, which is insufficient for using VECM, we will instead fit a VAR model for the annual frequency aggregated time series. Nevertheless, the subsequent section on cross-validation will elaborate on the precise sizing of the training dataset required to ensure compatibility with the VAR and VECM models.

On the other hand, for the seasonal components with repetitive patterns, we will use the seasonal naïve (Snaïve) method, which sets each forecast equal to the last observed value from the same season (e.g., the same month of the previous year).

Finally, the efficacy of this methodology will be assessed through time series cross-validation, utilizing two accuracy metrics:

* *Root Mean Square Error (RMSE)*, 
where the RMSE is computed as:

$\text{RMSE} = \sqrt{\frac{1}{n} \sum_{t=1}^{n} (y_t - \hat{y}_t)^2}$; and

* *Mean Absolute Percentage Error (MAPE)*,
where the MAPE is computed by defining the percentage error, to overcome scale-dependency of RMSE as:

$\text{MAPE} = \frac{1}{n} \sum_{t=1}^{n} \left| 100 \times \frac{y_t - \hat{y}_t}{y_t} \right|$

where $n$ represents the total number of observations.

# Time Series Cross-Validation

## Definition and rationale

Time series cross-validation represents a sophisticated adaptation of the conventional training/test set approach for model selection, as stated by @hyndman2021forecasting. This methodology is particularly well-suited to time series data because it exclusively includes observations from periods prior to those being forecasted, distinguishing it from traditional cross-validation techniques. 

In this study, time series cross-validation is employed to evaluate whether applying VAR/VECM with hierarchical forecasts yields more accurate results for this dataset across forecasting horizons ranging from 1 to 12 steps ahead, compared to the original forecast by the *DTF*. Additionally, this approach is used to gauge the extent of improvement introduced by the reconciliation process in comparison to the base forecasts. Time series cross-validation facilitates the generation of a series of 1 to 12-step-ahead forecasts across various rolling periods. For instance, a 12-step-ahead forecast can be generated starting from January 2022, followed by another 12-step-ahead forecast from February 2022, and so forth. This is essential for the *Department of Treasury and Finance* to analyze the effects of market dynamics or policy changes on land transfer duty. 

The initial training set will start with 111 monthly observations and will roll forward by 1 month for each subsequent step. This process will generate 10 folds of training sets, and thus there will be 10 sets of 12-step-ahead forecasts with equivalent test set for forecasting accuracy evaluation. While the number of observations in the initial training set can be lower, 111 months were chosen for code efficiency.

## Generating forecast

To enhance forecast accuracy, this methodology extends beyond a cross-sectional hierarchical structure to incorporate a temporal hierarchical structure, culminating in cross-temporal reconciliation forecasting. This approach integrates forecasts across different time structures, and its efficacy is evaluated by comparing it with cross-sectional and temporal reconciliation forecasts individually to determine any improvements.

Given the monthly frequency of the data, additional aggregation levels such as bi-monthly, quarterly, four-monthly, semi-annually, and annually are established, with the annual aggregation representing the top level of the temporal hierarchy.

Models are fitted at each cross-sectional level across these varying temporal frequencies. A loop is utilized to generate forecasts and residuals, which are subsequently organized into a matrix for each temporal frequency: monthly (denoted as k1), bi-monthly (k2), quarterly (k3), four-monthly (k4), semi-annually (k6), and annually (k12). These matrices are then collectively stored within a list.

This structured approach facilitates the fitting of different models tailored to each temporal frequency, allowing for adjustments in argument values as necessary. Moreover, if a distinct model is required for various cross-sectional levels, an `if` condition is employed within each temporal frequency loop to ensure this customization. The forecasts and residuals are then allocated to `base` and `res` data structures, respectively.

### Seasonal naïve method

As mentioned in @sec-model, we will use seasonal naïve method (Snaïve) for the seasonal components. Formally, the forecast for time $T + h$ is written as:

$$
\begin{aligned}
\hat{y}_{T+h|T} = y_{T+h-m(k+1)},
\end{aligned}
$$
where $m$ = the seasonal period, and $k$ is the integer part of $(h-1)/m$ (i.e., the number of complete years in the forecast period prior to time $T+h$). For example, with monthly data, the forecast for all future February values is equal to the last observed February value.

### VAR/VECM

A vector autoregression model of order 1, VAR(1) between 2 time series can be represented with:

$$
\begin{aligned}
y_t &= \beta_{10} + \beta_{11} y_{t-1} + \beta_{12} x_{t-1} + \nu_t^y \\
x_t &= \beta_{20} + \beta_{21} y_{t-1} + \beta_{22} x_{t-1} + \nu_t^x
\end{aligned}
$$
Then the one-step-ahead forecasts are generated by:

$$
\begin{aligned}
\hat{y}_{T+1|T} &= \hat{\beta}_{10} + \hat{\beta}_{11} y_{T} + \hat{\beta}_{12} x_{T} \\
\hat{x}_{T+1|T} &= \hat{\beta}_{20} + \hat{\beta}_{21} y_{T} + \hat{\beta}_{22} x_{T}
\end{aligned}
$$

The VAR(1) equation can be extended to lag of 3 between 3 time series, which is our case:

$$
\begin{aligned}
y_t &= \beta_{10} + \beta_{11} y_{t-1} + \beta_{12} x_{t-1} + \beta_{13} z_{t-1} + \beta_{14} y_{t-2} + \beta_{15} x_{t-2} + \beta_{16} z_{t-2} + \beta_{17} y_{t-3} + \beta_{18} x_{t-3} + \beta_{19} z_{t-3} + \nu_t^y \\
x_t &= \beta_{20} + \beta_{21} y_{t-1} + \beta_{22} x_{t-1} + \beta_{23} z_{t-1} + \beta_{24} y_{t-2} + \beta_{25} x_{t-2} + \beta_{26} z_{t-2} + \beta_{27} y_{t-3} + \beta_{28} x_{t-3} + \beta_{29} z_{t-3} + \nu_t^x \\
z_t &= \beta_{30} + \beta_{31} y_{t-1} + \beta_{32} x_{t-1} + \beta_{33} z_{t-1} + \beta_{34} y_{t-2} + \beta_{35} x_{t-2} + \beta_{36} z_{t-2} + \beta_{37} y_{t-3} + \beta_{38} x_{t-3} + \beta_{39} z_{t-3} + \nu_t^z
\end{aligned},
$$
where $y_t$, $x_t$ and $z_t$ represent land transfer duty, sales and home value index.

However, as discussed in @sec-coint, given the existence of cointegration relationship among the three time series, which are different type of property land transfer duty against sales and home value index, fitting a VECM is more appropriate. The VECM is formulated by incorporating the error correction term into the VAR model in first differences. For the three time series  $y_t$, $x_t$, and $z_t$, the VECM(3) can be expressed as:

$$
\begin{aligned}
\Delta y_t &= \alpha_1 (\gamma_{11} y_{t-1} + \gamma_{12} x_{t-1} + \gamma_{13} z_{t-1} - \mu_1) + \sum_{j=1}^{3} \left( \beta_{1j1} \Delta y_{t-j} + \beta_{1j2} \Delta x_{t-j} + \beta_{1j3} \Delta z_{t-j} \right) + \varepsilon_t^y \\
\Delta x_t &= \alpha_2 (\gamma_{21} y_{t-1} + \gamma_{22} x_{t-1} + \gamma_{23} z_{t-1} - \mu_2) + \sum_{j=1}^{3} \left( \beta_{2j1} \Delta y_{t-j} + \beta_{2j2} \Delta x_{t-j} + \beta_{2j3} \Delta z_{t-j} \right) + \varepsilon_t^x \\
\Delta z_t &= \alpha_3 (\gamma_{31} y_{t-1} + \gamma_{32} x_{t-1} + \gamma_{33} z_{t-1} - \mu_3) + \sum_{j=1}^{3} \left( \beta_{3j1} \Delta y_{t-j} + \beta_{3j2} \Delta x_{t-j} + \beta_{3j3} \Delta z_{t-j} \right) + \varepsilon_t^z
\end{aligned}
$$

where:

- $\Delta y_t$, $\Delta x_t$, and $\Delta z_t$ represent the first differences of $y_t$, $x_t$, and $z_t$, respectively.
- $\alpha_i$ are the adjustment coefficients for the error correction terms.
- $\gamma_{ij}$ are the coefficients of the cointegrating vectors.
- $\mu_i$ are the constants in the cointegration relationships.
- $\varepsilon_t^y$, $\varepsilon_t^x$, and $\varepsilon_t^z$ are the white noise error terms.

The VECM framework thus captures both the short-term dynamics through the differenced terms and the long-term equilibrium relationships through the error correction terms.

Finally, the reconciliation procedure can be implemented using the `FoReco` package (@FoReco).

# Results
## RMSE

```{r}
#| message: False
#| warning: False
#| echo: False
#| label: fig-rmse
#| fig-cap: "Monthly total LTD base and reconciled forecast RMSE by method"
#| fig-align: 'center'

i = 1
## Create a data frame for plotting
rmse_base_data <- data.frame(
  h_step = 1:12,
  RMSE_VECM_base = RMSE_vecm_h_base[i,],
  rmse_VECM_rec = RMSE_vecm_h_tcs_rec[i,],
  rmse_VECM_temp = RMSE_vecm_h_thf_rec[i,],
  rmse_VECM_cross_sec = RMSE_vecm_h_hts_rec[i,],
  rmse_DTF = dtf_rmse$rmse
)

## Convert to long format
rmse_data_long <- rmse_base_data %>% 
  pivot_longer(cols = -h_step, names_to = "Method", values_to = "RMSE")

## Add columns for line type and method
rmse_data_long <- rmse_data_long %>%
  mutate(
    LineType = case_when(
      grepl("rec", Method) ~ "Cross-temporal",
      grepl("temp", Method) ~ "Temporal",
      grepl("cross_sec", Method) ~ "Cross-sectional",
      TRUE ~ "Base"
    ),
    MethodGroup = case_when(
      grepl("DTF", Method) ~ "DTF",
      grepl("VECM", Method) ~ "Forecast Reconciliation"
    )
  )

## Create the plot with specified colors, line types, and markers
ggplot_rmse <- ggplot(rmse_data_long, aes(x = h_step, y = RMSE, colour = MethodGroup, linetype = LineType, shape = LineType)) +
  geom_line(aes(group = Method), linewidth = 1.2) +
  geom_point(data = subset(rmse_data_long, LineType == "Temporal"), size = 2) +
  scale_color_manual(values = c(
    "DTF" = "blue",
    "Forecast Reconciliation" = "red"
  )) +
  scale_linetype_manual(values = c(
    "Base" = "solid",
    "Cross-temporal" = "dashed",
    "Temporal" = "solid",
    "Cross-sectional" = "dotted"
  )) +
  scale_shape_manual(values = c(
    "Base" = NA,
    "Cross-temporal" = NA,
    "Temporal" = 16,
    "Cross-sectional" = NA
  )) +
  labs(x = "h-step Forecast", y = "RMSE Value", title = " ") +
  scale_x_continuous(breaks = 1:12) +
  theme_bw() +
  guides(
    colour = guide_legend(title = "Method", order = 1),
    linetype = guide_legend(title = "Reconciliation Type", order = 2),
    shape = guide_legend(title = "Reconciliation Type", order = 2)
  ) +
  theme(
    legend.key = element_rect(fill = "white"),
    legend.key.size = unit(1.2, "lines"),
    legend.spacing.x = unit(0.5, "cm"),
    legend.spacing.y = unit(0.5, "cm")
  )

ggplot_rmse

```


@fig-rmse illustrates the Root Mean Square Error (RMSE) of reconciled forecasts and base forecasts compared against the RMSE of DTF forecasts. The blue line denotes the DTF forecast, while the red lines represent hierarchical reconciled forecasts. The line types are differentiated as follows: solid lines for base forecasts, solid lines with circle markers for temporally reconciled forecasts, dotted lines for cross-sectionally reconciled forecasts, and dashed lines for cross-temporally reconciled forecasts.

It is clear that the significant disparity between reconciled forecasts and base forecasts, particularly from the four-step-ahead forecasts onwards, suggests that forecast reconciliation markedly enhances forecast accuracy. Notably, the RMSE line of temporally reconciled forecasts is closer to the cross-temporally reconciled forecasts compared to the cross-sectionally reconciled forecasts. This indicates that temporal hierarchies play a more substantial role in improving forecast accuracy compared to cross-sectional hierarchies.

Moreover, when comparing reconciled forecasts to DTF forecasts, it is evident that reconciled forecasts, especially cross-temporally, perform significantly better.

It is also noteworthy that for one to three-step-ahead forecasts, the base forecasts already exhibit high performance, hence the improvement from applying forecast reconciliation is not as evident.

## MAPE 

Using the Mean Absolute Percentage Error (MAPE), we anticipate results consistent with those obtained from the Root Mean Square Error (RMSE), which is the case as demonstrated in @fig-mape.

```{r}
#| message: False
#| warning: False
#| echo: False
#| label: fig-mape
#| fig-cap: "Monthly total LTD base and reconciled forecast MAPE by Method"
#| fig-align: 'center'
#| 

i = 1
## Create a data frame for plotting
mape_base_data <- data.frame(
  h_step = 1:12,
  mape_vecm_base = mape_vecm_base[i,],
  mape_vecm_rec = mape_vecm_cross_temp[i,],
  mape_vecm_temp = mape_vecm_temp[i,],
  mape_vecm_cross_sec = mape_vecm_cross_sec[i,],
  mape_dtf = dtf_mape$mape
)

## Convert to long format
mape_data_long <- mape_base_data %>% 
  pivot_longer(cols = -h_step, names_to = "Method", values_to = "MAPE")

## Add columns for line type and method
mape_data_long <- mape_data_long %>%
  mutate(
    LineType = case_when(
      grepl("rec", Method) ~ "Cross-temporal",
      grepl("temp", Method) ~ "Temporal",
      grepl("cross_sec", Method) ~ "Cross-sectional",
      TRUE ~ "Base"
    ),
    MethodGroup = case_when(
      grepl("dtf", Method) ~ "DTF",
      grepl("vecm", Method) ~ "Forecast Reconciliation"
    )
  )

## Create the plot with specified colors, line types, and markers
ggplot_mape <- ggplot(mape_data_long, aes(x = h_step, y = MAPE, colour = MethodGroup, linetype = LineType, shape = LineType)) +
  geom_line(aes(group = Method), linewidth = 1.2) +
  geom_point(data = subset(mape_data_long, LineType == "Temporal"), size = 2) +
  scale_color_manual(values = c(
    "DTF" = "blue",
    "Forecast Reconciliation" = "red"
  )) +
  scale_linetype_manual(values = c(
    "Base" = "solid",
    "Cross-temporal" = "dashed",
    "Temporal" = "solid",
    "Cross-sectional" = "dotted"
  )) +
  scale_shape_manual(values = c(
    "Base" = NA,
    "Cross-temporal" = NA,
    "Temporal" = 16,
    "Cross-sectional" = NA
  )) +
  labs(x = "h-step Forecast", y = "MAPE Value", title = " ") +
  scale_x_continuous(breaks = 1:12) +
  theme_bw() +
  guides(
    colour = guide_legend(title = "Method", order = 1),
    linetype = guide_legend(title = "Reconciliation Type", order = 2),
    shape = guide_legend(title = "Reconciliation Type", order = 2)
  ) +
  theme(
    legend.key = element_rect(fill = "white"),
    legend.key.size = unit(1.2, "lines"),
    legend.spacing.x = unit(0.5, "cm"),
    legend.spacing.y = unit(0.5, "cm")
  )

ggplot_mape
```

# Discussion
## Interpretation of Results

@fig-rmse and @fig-mape reveals three key insights that underscore the effectiveness of forecast reconciliation in improving accuracy:

1. **Enhanced Forecast Accuracy**: The integration of both cross-sectional and temporal hierarchies within a unified cross-temporal reconciliation framework significantly improves forecast accuracy compared to the DTF forecasts. Among the reconciled forecasts, those that incorporate cross-temporal reconciliation exhibit the best performance, consistently achieving lower RMSE values.

2. **Impact of Temporal Hierarchies**: The primary driver of the improved accuracy is the incorporation of temporal hierarchies. Temporal reconciliation proves to be more influential than cross-sectional reconciliation.

3. **Improvement of Poor-Performing Forecasts**: The reconciliation process is particularly effective in improving forecasts that initially performed poorly.

## Limitations

When fitting the VECM with variables such as sales and the Home Value Index (HVI) or lending and sales trends relative to total LTD, an inconsistency emerged: the Augmented Dickey-Fuller (ADF) test on the linear combination of these series, derived from the eigenvector components in the Johansen test, indicated non-stationarity. This result conflicts with the Johansen test, which suggests the existence of a cointegrating relationship. As a result, further research is recommended to investigate this issue by delving deeper into its underlying causes, such as model specification, the presence of structural breaks, or the need for alternative testing methods.

# Conclusion

This paper introduces a novel approach to forecasting land transfer duty (LTD) in Victoria, utilizing forecast reconciliation techniques within cross-sectional, temporal, and cross-temporal hierarchies. Our results show that this methodology significantly improves forecast accuracy, particularly when compared to the Department of Treasury and Finance (DTF) benchmarks.

The findings emphasize the importance of temporal hierarchies in driving these improvements and demonstrate the method's effectiveness in enhancing forecasts that initially perform poorly. Despite the observed conflict in cointegration analysis, the proposed approach offers valuable insights for improving LTD forecasts.

In summary, while the methodology shows strong potential for accurate forecasting, further research is recommended to address the identified limitations and refine the approach for even better results.